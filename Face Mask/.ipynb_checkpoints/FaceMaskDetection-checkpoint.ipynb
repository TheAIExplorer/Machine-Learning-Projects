{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c2ee4193-b35f-4333-a0c6-a0a687aa9e55",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bounding Boxes Shapes: [(3, 4), (9, 4), (1, 4), (3, 4), (1, 4), (1, 4), (4, 4), (8, 4), (9, 4), (1, 4), (1, 4), (5, 4), (1, 4), (16, 4), (26, 4), (12, 4), (1, 4), (4, 4), (2, 4), (9, 4), (5, 4), (1, 4), (2, 4), (7, 4), (13, 4), (1, 4), (7, 4), (1, 4), (2, 4), (5, 4), (1, 4), (3, 4), (1, 4), (1, 4), (3, 4), (6, 4), (4, 4), (8, 4), (1, 4), (1, 4), (7, 4), (2, 4), (1, 4), (6, 4), (3, 4), (19, 4), (1, 4), (2, 4), (2, 4), (1, 4), (1, 4), (3, 4), (3, 4), (2, 4), (4, 4), (6, 4), (6, 4), (2, 4), (1, 4), (10, 4), (7, 4), (2, 4), (1, 4), (4, 4), (13, 4), (8, 4), (14, 4), (12, 4), (1, 4), (2, 4), (1, 4), (1, 4), (2, 4), (9, 4), (2, 4), (6, 4), (4, 4), (5, 4), (1, 4), (1, 4), (1, 4), (1, 4), (1, 4), (2, 4), (4, 4), (2, 4), (11, 4), (2, 4), (3, 4), (4, 4), (2, 4), (1, 4), (3, 4), (2, 4), (4, 4), (10, 4), (4, 4), (1, 4), (1, 4), (1, 4), (3, 4), (6, 4), (1, 4), (1, 4), (1, 4), (2, 4), (2, 4), (1, 4), (1, 4), (1, 4), (1, 4), (5, 4), (4, 4), (1, 4), (3, 4), (10, 4), (6, 4), (4, 4), (7, 4), (1, 4), (8, 4), (4, 4), (2, 4), (14, 4), (5, 4), (1, 4), (1, 4), (1, 4), (1, 4), (7, 4), (2, 4), (5, 4), (1, 4), (1, 4), (2, 4), (1, 4), (1, 4), (10, 4), (17, 4), (14, 4), (1, 4), (1, 4), (1, 4), (12, 4), (1, 4), (14, 4), (1, 4), (5, 4), (15, 4), (5, 4), (1, 4), (5, 4), (4, 4), (1, 4), (2, 4), (10, 4), (1, 4), (1, 4), (61, 4), (1, 4), (1, 4), (1, 4), (3, 4), (7, 4), (4, 4), (13, 4), (1, 4), (3, 4), (1, 4), (5, 4), (24, 4), (1, 4), (10, 4), (7, 4), (19, 4), (13, 4), (8, 4), (1, 4), (1, 4), (2, 4), (53, 4), (6, 4), (1, 4), (3, 4), (7, 4), (6, 4), (1, 4), (4, 4), (4, 4), (2, 4), (1, 4), (6, 4), (1, 4), (1, 4), (1, 4), (1, 4), (1, 4), (2, 4), (11, 4), (4, 4), (3, 4), (1, 4), (21, 4), (1, 4), (9, 4), (1, 4), (7, 4), (1, 4), (2, 4), (2, 4), (3, 4), (2, 4), (3, 4), (7, 4), (1, 4), (2, 4), (4, 4), (10, 4), (1, 4), (35, 4), (16, 4), (2, 4), (1, 4), (9, 4), (7, 4), (1, 4), (16, 4), (3, 4), (2, 4), (2, 4), (14, 4), (1, 4), (3, 4), (3, 4), (1, 4), (2, 4), (6, 4), (5, 4), (3, 4), (1, 4), (9, 4), (2, 4), (4, 4), (2, 4), (7, 4), (11, 4), (2, 4), (4, 4), (2, 4), (1, 4), (1, 4), (1, 4), (11, 4), (2, 4), (7, 4), (1, 4), (9, 4), (1, 4), (4, 4), (15, 4), (1, 4), (1, 4), (4, 4), (1, 4), (8, 4), (1, 4), (5, 4), (1, 4), (8, 4), (4, 4), (1, 4), (13, 4), (1, 4), (4, 4), (2, 4), (4, 4), (3, 4), (1, 4), (3, 4), (2, 4), (2, 4), (1, 4), (2, 4), (1, 4), (25, 4), (1, 4), (1, 4), (1, 4), (4, 4), (18, 4), (9, 4), (1, 4), (3, 4), (3, 4), (6, 4), (3, 4), (3, 4), (2, 4), (1, 4), (9, 4), (1, 4), (1, 4), (1, 4), (1, 4), (2, 4), (13, 4), (5, 4), (2, 4), (7, 4), (1, 4), (2, 4), (1, 4), (2, 4), (1, 4), (6, 4), (5, 4), (1, 4), (4, 4), (5, 4), (9, 4), (4, 4), (17, 4), (12, 4), (2, 4), (6, 4), (4, 4), (1, 4), (2, 4), (1, 4), (1, 4), (5, 4), (3, 4), (1, 4), (8, 4), (1, 4), (4, 4), (3, 4), (4, 4), (1, 4), (1, 4), (1, 4), (1, 4), (1, 4), (1, 4), (10, 4), (1, 4), (9, 4), (21, 4), (8, 4), (2, 4), (3, 4), (4, 4), (1, 4), (4, 4), (1, 4), (2, 4), (8, 4), (6, 4), (1, 4), (5, 4), (2, 4), (13, 4), (22, 4), (1, 4), (1, 4), (4, 4), (21, 4), (4, 4), (2, 4), (2, 4), (13, 4), (1, 4), (1, 4), (1, 4), (5, 4), (2, 4), (5, 4), (4, 4), (4, 4), (4, 4), (8, 4), (3, 4), (1, 4), (1, 4), (4, 4), (10, 4), (1, 4), (3, 4), (4, 4), (4, 4), (3, 4), (4, 4), (13, 4), (3, 4), (23, 4), (1, 4), (1, 4), (1, 4), (6, 4), (3, 4), (2, 4), (10, 4), (12, 4), (9, 4), (1, 4), (1, 4), (4, 4), (1, 4), (4, 4), (6, 4), (1, 4), (2, 4), (1, 4), (5, 4), (5, 4), (1, 4), (4, 4), (4, 4), (6, 4), (7, 4), (13, 4), (2, 4), (1, 4), (1, 4), (1, 4), (5, 4), (1, 4), (4, 4), (1, 4), (1, 4), (8, 4), (5, 4), (2, 4), (8, 4), (8, 4), (1, 4), (1, 4), (1, 4), (1, 4), (1, 4), (9, 4), (1, 4), (1, 4), (11, 4), (2, 4), (4, 4), (2, 4), (1, 4), (6, 4), (4, 4), (2, 4), (1, 4), (1, 4), (1, 4), (1, 4), (4, 4), (1, 4), (1, 4), (4, 4), (1, 4), (7, 4), (3, 4), (10, 4), (1, 4), (7, 4), (4, 4), (83, 4), (3, 4), (51, 4), (1, 4), (1, 4), (3, 4), (2, 4), (18, 4), (1, 4), (4, 4), (6, 4), (1, 4), (8, 4), (3, 4), (1, 4), (1, 4), (1, 4), (1, 4), (11, 4), (6, 4), (1, 4), (1, 4), (3, 4), (41, 4), (4, 4), (1, 4), (1, 4), (1, 4), (10, 4), (1, 4), (13, 4), (1, 4), (3, 4), (2, 4), (1, 4), (1, 4), (1, 4), (5, 4), (1, 4), (1, 4), (5, 4), (4, 4), (10, 4), (1, 4), (17, 4), (15, 4), (1, 4), (1, 4), (1, 4), (1, 4), (1, 4), (23, 4), (1, 4), (7, 4), (5, 4), (1, 4), (1, 4), (1, 4), (8, 4), (1, 4), (6, 4), (6, 4), (6, 4), (5, 4), (2, 4), (11, 4), (3, 4), (3, 4), (13, 4), (1, 4), (1, 4), (1, 4), (3, 4), (10, 4), (6, 4), (3, 4), (2, 4), (3, 4), (1, 4), (5, 4), (1, 4), (1, 4), (1, 4), (3, 4), (8, 4), (6, 4), (1, 4), (2, 4), (1, 4), (1, 4), (1, 4), (1, 4), (1, 4), (1, 4), (1, 4), (115, 4), (2, 4), (1, 4), (2, 4), (2, 4), (1, 4), (1, 4), (13, 4), (3, 4), (5, 4), (1, 4), (10, 4), (1, 4), (1, 4), (15, 4), (1, 4), (1, 4), (3, 4), (1, 4), (6, 4), (5, 4), (5, 4), (23, 4), (1, 4), (1, 4), (1, 4), (14, 4), (2, 4), (1, 4), (1, 4), (1, 4), (11, 4), (1, 4), (1, 4), (4, 4), (1, 4), (5, 4), (6, 4), (2, 4), (4, 4), (11, 4), (8, 4), (4, 4), (2, 4), (1, 4), (1, 4), (1, 4), (1, 4), (12, 4), (3, 4), (1, 4), (3, 4), (1, 4), (1, 4), (1, 4), (2, 4), (1, 4), (1, 4), (20, 4), (13, 4), (1, 4), (1, 4), (1, 4), (1, 4), (15, 4), (4, 4), (1, 4), (1, 4), (8, 4), (3, 4), (3, 4), (1, 4), (1, 4), (4, 4), (4, 4), (1, 4), (9, 4), (5, 4), (18, 4), (1, 4), (11, 4), (1, 4), (2, 4), (1, 4), (1, 4), (10, 4), (1, 4), (5, 4), (1, 4), (5, 4), (4, 4), (1, 4), (1, 4), (1, 4), (1, 4), (1, 4), (20, 4), (11, 4), (1, 4), (2, 4), (6, 4), (28, 4), (8, 4), (2, 4), (7, 4), (7, 4), (4, 4), (5, 4), (2, 4), (1, 4), (11, 4), (10, 4), (8, 4), (16, 4), (17, 4), (1, 4), (3, 4), (1, 4), (6, 4), (3, 4), (8, 4), (1, 4), (1, 4), (4, 4), (2, 4), (1, 4), (4, 4), (9, 4), (9, 4), (1, 4), (5, 4), (1, 4), (9, 4), (13, 4), (1, 4), (1, 4), (4, 4), (1, 4), (6, 4), (1, 4), (3, 4), (3, 4), (2, 4), (8, 4), (1, 4), (1, 4), (11, 4), (2, 4), (6, 4), (2, 4), (1, 4), (26, 4), (2, 4), (1, 4), (1, 4), (1, 4), (2, 4), (1, 4), (1, 4), (1, 4), (1, 4), (2, 4), (13, 4), (7, 4), (4, 4), (11, 4), (16, 4), (4, 4), (8, 4), (2, 4), (1, 4), (5, 4), (3, 4), (4, 4), (7, 4), (1, 4), (1, 4), (2, 4), (1, 4), (1, 4), (1, 4), (1, 4), (6, 4), (2, 4), (3, 4), (8, 4), (5, 4), (2, 4), (17, 4), (3, 4), (2, 4), (2, 4), (8, 4), (5, 4), (5, 4), (1, 4), (8, 4), (1, 4), (2, 4), (5, 4), (9, 4), (1, 4), (4, 4), (8, 4), (5, 4), (4, 4), (5, 4), (1, 4), (1, 4), (6, 4), (1, 4), (2, 4), (21, 4), (13, 4), (8, 4), (2, 4), (7, 4), (1, 4), (1, 4), (4, 4), (8, 4), (4, 4), (1, 4), (12, 4), (9, 4), (6, 4), (9, 4), (1, 4), (3, 4), (3, 4), (1, 4), (1, 4), (3, 4), (1, 4), (1, 4), (1, 4), (5, 4), (1, 4), (1, 4), (1, 4), (6, 4), (1, 4), (2, 4), (14, 4), (4, 4), (8, 4), (4, 4), (8, 4), (1, 4), (1, 4), (12, 4), (3, 4), (1, 4), (1, 4), (1, 4), (1, 4), (5, 4), (1, 4), (5, 4), (1, 4), (1, 4), (4, 4), (3, 4), (2, 4), (1, 4), (4, 4), (3, 4), (2, 4), (1, 4), (1, 4), (19, 4), (4, 4), (4, 4), (1, 4), (3, 4), (1, 4), (2, 4), (4, 4), (1, 4), (1, 4), (29, 4), (2, 4), (8, 4), (5, 4), (8, 4), (9, 4), (6, 4), (1, 4), (4, 4), (1, 4), (4, 4), (3, 4)]\n",
      "Padded Bounding Boxes Shapes: [(115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4), (115, 4)]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "operands could not be broadcast together with shapes (115,4) (0,) ",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[31], line 112\u001b[0m\n\u001b[0;32m    110\u001b[0m \u001b[38;5;66;03m# Pad bounding_boxes sequences to the same length\u001b[39;00m\n\u001b[0;32m    111\u001b[0m max_boxes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmax\u001b[39m(\u001b[38;5;28mlen\u001b[39m(bboxes) \u001b[38;5;28;01mfor\u001b[39;00m bboxes \u001b[38;5;129;01min\u001b[39;00m bounding_boxes)\n\u001b[1;32m--> 112\u001b[0m padded_bounding_boxes \u001b[38;5;241m=\u001b[39m [bboxes \u001b[38;5;241m+\u001b[39m [[\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m]] \u001b[38;5;241m*\u001b[39m (max_boxes \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mlen\u001b[39m(bboxes)) \u001b[38;5;28;01mfor\u001b[39;00m bboxes \u001b[38;5;129;01min\u001b[39;00m bounding_boxes]\n\u001b[0;32m    114\u001b[0m \u001b[38;5;66;03m# Convert padded_bounding_boxes to TensorFlow Tensor\u001b[39;00m\n\u001b[0;32m    115\u001b[0m bounding_boxes \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mconvert_to_tensor(padded_bounding_boxes, dtype\u001b[38;5;241m=\u001b[39mtf\u001b[38;5;241m.\u001b[39mfloat32)\n",
      "Cell \u001b[1;32mIn[31], line 112\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    110\u001b[0m \u001b[38;5;66;03m# Pad bounding_boxes sequences to the same length\u001b[39;00m\n\u001b[0;32m    111\u001b[0m max_boxes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmax\u001b[39m(\u001b[38;5;28mlen\u001b[39m(bboxes) \u001b[38;5;28;01mfor\u001b[39;00m bboxes \u001b[38;5;129;01min\u001b[39;00m bounding_boxes)\n\u001b[1;32m--> 112\u001b[0m padded_bounding_boxes \u001b[38;5;241m=\u001b[39m [\u001b[43mbboxes\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mmax_boxes\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mbboxes\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m bboxes \u001b[38;5;129;01min\u001b[39;00m bounding_boxes]\n\u001b[0;32m    114\u001b[0m \u001b[38;5;66;03m# Convert padded_bounding_boxes to TensorFlow Tensor\u001b[39;00m\n\u001b[0;32m    115\u001b[0m bounding_boxes \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mconvert_to_tensor(padded_bounding_boxes, dtype\u001b[38;5;241m=\u001b[39mtf\u001b[38;5;241m.\u001b[39mfloat32)\n",
      "\u001b[1;31mValueError\u001b[0m: operands could not be broadcast together with shapes (115,4) (0,) "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Flatten, Dense, BatchNormalization\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.applications.resnet50 import ResNet50\n",
    "import xml.etree.ElementTree as ET\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "\n",
    "# Avoid OOM errors by setting GPU Memory Consumption Growth\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "for gpu in gpus:\n",
    "    tf.config.experimental.set_memory_growth(gpu, True)\n",
    "\n",
    "def read_annotation_file(annotation_file_path):\n",
    "    tree = ET.parse(annotation_file_path)\n",
    "    root = tree.getroot()\n",
    "    \n",
    "    size = root.find('size')\n",
    "    width = int(size.find('width').text)\n",
    "    height = int(size.find('height').text)\n",
    "\n",
    "    annotations = []\n",
    "    for obj in root.findall('object'):\n",
    "        name = obj.find('name').text\n",
    "        bndbox = obj.find('bndbox')\n",
    "        xmin = int(bndbox.find('xmin').text)\n",
    "        ymin = int(bndbox.find('ymin').text)\n",
    "        xmax = int(bndbox.find('xmax').text)\n",
    "        ymax = int(bndbox.find('ymax').text)\n",
    "\n",
    "        annotation_data = {\n",
    "            'class': name,\n",
    "            'xmin': xmin,\n",
    "            'ymin': ymin,\n",
    "            'xmax': xmax,\n",
    "            'ymax': ymax\n",
    "        }\n",
    "        annotations.append(annotation_data)\n",
    "\n",
    "    return annotations\n",
    "\n",
    "def preprocess_data(data):\n",
    "    processed_data = []\n",
    "    class_labels = set()\n",
    "    bounding_boxes = []  # Using a list of lists\n",
    "\n",
    "    for item in data:\n",
    "        image = item['image_data']\n",
    "        annotations = item['annotations']\n",
    "\n",
    "        # Resize the image to a uniform size (e.g., 224x224) for CNN input\n",
    "        image = cv2.resize(image, (224, 224))\n",
    "\n",
    "        # Normalize pixel values to the range [0, 1]\n",
    "        image = image.astype(np.float32) / 255.0\n",
    "\n",
    "        # Preprocess annotations - One-hot encode class labels\n",
    "        classes = [annotation['class'] for annotation in annotations]\n",
    "        class_labels.update(classes)  # Add class labels to the set of unique labels\n",
    "\n",
    "        # Append image file path and annotations to the data list\n",
    "        processed_data.append({'image_data': image, 'annotations': classes})\n",
    "\n",
    "        # Append bounding box coordinates to the bounding_boxes list for each image\n",
    "        boxes = [[annotation['xmin'], annotation['ymin'], annotation['xmax'], annotation['ymax']] for annotation in annotations]\n",
    "        bounding_boxes.append(boxes)\n",
    "\n",
    "    # Print the shapes of bounding_boxes\n",
    "    print(\"Bounding Boxes Shapes:\", [np.array(bboxes).shape for bboxes in bounding_boxes])\n",
    "\n",
    "    # Pad bounding_boxes sequences to the same length\n",
    "    max_boxes = max(len(bboxes) for bboxes in bounding_boxes)\n",
    "    padded_bounding_boxes = []\n",
    "    for bboxes in bounding_boxes:\n",
    "        if len(bboxes) < max_boxes:\n",
    "            bboxes += [[0, 0, 0, 0]] * (max_boxes - len(bboxes))\n",
    "        padded_bounding_boxes.append(bboxes)\n",
    "\n",
    "    # Print the shapes of padded_bounding_boxes\n",
    "    print(\"Padded Bounding Boxes Shapes:\", [np.array(bboxes).shape for bboxes in padded_bounding_boxes])\n",
    "\n",
    "    return processed_data, list(class_labels), padded_bounding_boxes\n",
    "\n",
    "\n",
    "# Preprocess the data\n",
    "processed_data, class_labels, bounding_boxes = preprocess_data(data)\n",
    "\n",
    "# Convert the list of dictionaries into separate arrays\n",
    "images = np.array([item['image_data'] for item in processed_data])\n",
    "\n",
    "# Convert annotations to a flat list of lists\n",
    "annotations = [item['annotations'] for item in processed_data]\n",
    "\n",
    "# Initialize and fit the MultiLabelBinarizer\n",
    "mlb = MultiLabelBinarizer()\n",
    "annotations_encoded = mlb.fit_transform(annotations)\n",
    "\n",
    "# Convert annotations_encoded and bounding_boxes to numpy arrays\n",
    "annotations_encoded = np.array(annotations_encoded)\n",
    "bounding_boxes = np.array(bounding_boxes)\n",
    "\n",
    "# Convert annotations_encoded to TensorFlow Tensor\n",
    "annotations_encoded = tf.convert_to_tensor(annotations_encoded, dtype=tf.float32)\n",
    "\n",
    "# Pad bounding_boxes sequences to the same length\n",
    "max_boxes = max(len(bboxes) for bboxes in bounding_boxes)\n",
    "padded_bounding_boxes = [bboxes + [[0, 0, 0, 0]] * (max_boxes - len(bboxes)) for bboxes in bounding_boxes]\n",
    "\n",
    "# Convert padded_bounding_boxes to TensorFlow Tensor\n",
    "bounding_boxes = tf.convert_to_tensor(padded_bounding_boxes, dtype=tf.float32)\n",
    "\n",
    "# Split the data into training and testing sets using numpy.split\n",
    "data_indices = np.arange(len(images))\n",
    "train_indices, test_indices = train_test_split(data_indices, test_size=0.2, random_state=42, stratify=None)\n",
    "\n",
    "# Convert train_indices and test_indices to TensorFlow Tensors\n",
    "train_indices = tf.convert_to_tensor(train_indices, dtype=tf.int32)\n",
    "test_indices = tf.convert_to_tensor(test_indices, dtype=tf.int32)\n",
    "\n",
    "train_images = tf.gather(images, train_indices)\n",
    "test_images = tf.gather(images, test_indices)\n",
    "\n",
    "train_annotations_encoded = tf.gather(annotations_encoded, train_indices)\n",
    "test_annotations_encoded = tf.gather(annotations_encoded, test_indices)\n",
    "\n",
    "train_bounding_boxes = tf.gather(bounding_boxes, train_indices)\n",
    "test_bounding_boxes = tf.gather(bounding_boxes, test_indices)\n",
    "\n",
    "# Create the model using ResNet50 as the base model\n",
    "base_model = ResNet50(include_top=False, weights='imagenet', input_shape=(224, 224, 3))\n",
    "\n",
    "# Add custom layers for the classification task\n",
    "x = Flatten()(base_model.output)\n",
    "x = BatchNormalization()(x)\n",
    "class_output = Dense(len(class_labels), activation='sigmoid', name='class_output')(x)\n",
    "\n",
    "# Add custom layers for the bounding box regression task\n",
    "x = Flatten()(base_model.output)\n",
    "x = BatchNormalization()(x)\n",
    "box_output = Dense(4, activation='linear', name='box_output')(x)\n",
    "\n",
    "# Combine the class and box outputs into a single model\n",
    "model = Model(inputs=base_model.input, outputs=[class_output, box_output])\n",
    "\n",
    "# Define losses for the two outputs\n",
    "class_loss = tf.keras.losses.BinaryCrossentropy()\n",
    "box_loss = tf.keras.losses.MeanSquaredError()\n",
    "\n",
    "# Compile the model with custom losses for the two outputs\n",
    "model.compile(optimizer=Adam(learning_rate=0.001),\n",
    "              loss={'class_output': class_loss, 'box_output': box_loss},\n",
    "              metrics={'class_output': 'accuracy'})\n",
    "\n",
    "# Define early stopping callback to prevent overfitting\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "\n",
    "# Train the model\n",
    "model.fit(train_images, {'class_output': train_annotations_encoded, 'box_output': train_bounding_boxes},\n",
    "          validation_split=0.2, batch_size=8, epochs=20, callbacks=[early_stopping])\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "loss, class_loss, box_loss, class_accuracy = model.evaluate(test_images,\n",
    "                                                            {'class_output': test_annotations_encoded,\n",
    "                                                             'box_output': test_bounding_boxes})\n",
    "print(\"Test Total Loss:\", loss)\n",
    "print(\"Test Classification Loss:\", class_loss)\n",
    "print(\"Test Bounding Box Loss:\", box_loss)\n",
    "print(\"Test Classification Accuracy:\", class_accuracy)\n",
    "\n",
    "# Load and preprocess the image for prediction\n",
    "def preprocess_image(image_path):\n",
    "    image = cv2.imread(image_path)\n",
    "    image = cv2.resize(image, (224, 224))\n",
    "    image = image.astype(np.float32) / 255.0\n",
    "    return image\n",
    "\n",
    "# Path to the image you want to make a prediction on\n",
    "image_path = r'C:\\Users\\haris\\ArtificialIntelligence\\MachineLearning\\Projects\\Face Mask\\FaceMask/maksssksksss21.png'\n",
    "\n",
    "# Preprocess the image\n",
    "input_image = preprocess_image(image_path)\n",
    "\n",
    "# Expand dimensions to create a batch size of 1\n",
    "input_image = np.expand_dims(input_image, axis=0)\n",
    "\n",
    "# Make predictions using the model\n",
    "predictions = model.predict(input_image)\n",
    "class_predictions, box_predictions = predictions\n",
    "\n",
    "# Determine the class label with the highest probability\n",
    "predicted_class_indices = np.argmax(class_predictions, axis=1)\n",
    "predicted_class_labels = mlb.classes_[predicted_class_indices][0]  # Take the first class label if there are multiple\n",
    "\n",
    "# Load the image with OpenCV\n",
    "original_image = cv2.imread(image_path)\n",
    "\n",
    "# Get the bounding box coordinates from the XML file\n",
    "xml_file_path = image_path.replace('.png', '.xml')\n",
    "annotations = read_annotation_file(xml_file_path)\n",
    "for annotation in annotations:\n",
    "    if annotation['class'] == predicted_class_labels:\n",
    "        xmin = annotation['xmin']\n",
    "        ymin = annotation['ymin']\n",
    "        xmax = annotation['xmax']\n",
    "        ymax = annotation['ymax']\n",
    "        break\n",
    "\n",
    "# Draw the bounding box on the image\n",
    "color = (0, 255, 0)  # Green color for the bounding box\n",
    "thickness = 2\n",
    "cv2.rectangle(original_image, (xmin, ymin), (xmax, ymax), color, thickness)\n",
    "\n",
    "# Display or save the image with the bounding box\n",
    "cv2.imshow('Image with Bounding Box', original_image)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a7ceade-be78-46c8-8609-23aab16d432e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cf4aa32-4dda-4809-8442-74f18b394ee2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10 (tensorflow-gpu)",
   "language": "python",
   "name": "tensorflow-gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
