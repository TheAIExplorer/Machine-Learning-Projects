{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ffd85dc0-e563-489b-a1c5-a88a27d3d82e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Flatten, Dense, BatchNormalization\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.applications.resnet50 import ResNet50\n",
    "from tensorflow.keras.utils import Sequence\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e44d2f4e-1589-42e8-a05d-e50f78dcfd56",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Avoid OOM errors by setting GPU Memory Consumption Growth\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "for gpu in gpus:\n",
    "    tf.config.experimental.set_memory_growth(gpu, True)\n",
    "\n",
    "# Function to read annotation data from the .cat files\n",
    "def read_annotation_file(annotation_file_path):\n",
    "    with open(annotation_file_path, 'r') as file:\n",
    "        line = file.readline().strip()\n",
    "        values = line.split()\n",
    "        num_points = int(values[0])\n",
    "        annotation_data = [int(value) for value in values[1:]]\n",
    "        return num_points, annotation_data\n",
    "\n",
    "# Path to the directory containing the images and .cat files\n",
    "folder_path = r'C:\\Users\\haris\\AI\\ML\\Projects\\Data\\CatFaceFeatures\\Cats - Original'\n",
    "\n",
    "# Lists to store paths of .jpg files and .cat files\n",
    "image_files_path = []\n",
    "annotation_files_path = []\n",
    "\n",
    "for root, dirs, files in os.walk(folder_path):\n",
    "    for file in files:\n",
    "        if file.lower().endswith(('.jpg', '.jpeg', 'png')):\n",
    "            image_path = os.path.join(root, file)\n",
    "            image_files_path.append(image_path)\n",
    "\n",
    "        if file.lower().endswith('.cat'):\n",
    "            annotation_path = os.path.join(root, file)\n",
    "            annotation_files_path.append(annotation_path)\n",
    "\n",
    "# Lists to store resized image data and annotation data\n",
    "resized_images = []\n",
    "resized_annotations = []\n",
    "\n",
    "image_size = (224, 224)\n",
    "\n",
    "# Read annotation data for each image and apply normalization\n",
    "for annotation_file_path in annotation_files_path:\n",
    "    num_points, annotation_data = read_annotation_file(annotation_file_path)\n",
    "\n",
    "    image = cv2.imread(annotation_file_path[:-4], cv2.IMREAD_COLOR)\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    resized_image = cv2.resize(image, image_size)\n",
    "\n",
    "    # Calculate the scaling factor for annotation coordinates\n",
    "    scale_y = resized_image.shape[0] / image.shape[0]\n",
    "    scale_x = resized_image.shape[1] / image.shape[1]\n",
    "\n",
    "    # Apply scaling to the annotation data for both x and y coordinates together\n",
    "    resized_annotation_data = [int(value * scale_x) if i % 2 == 0 else int(value * scale_y) for i, value in enumerate(annotation_data)]\n",
    "\n",
    "    resized_images.append(resized_image)\n",
    "    resized_annotations.append(resized_annotation_data)\n",
    "\n",
    "# Convert lists to numpy arrays\n",
    "resized_images = np.array(resized_images)\n",
    "resized_annotations = np.array(resized_annotations)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6d1f64a7-c02d-4878-ac58-be5fb0ed994d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Split the data into training and testing sets\n",
    "x_train, x_test, y_train, y_test = train_test_split(resized_images, resized_annotations, test_size=0.1, random_state=42)\n",
    "\n",
    "# Function to load and preprocess the images and annotations in batches\n",
    "class DataGenerator(Sequence):\n",
    "    def __init__(self, x_set, y_set, batch_size):\n",
    "        self.x, self.y = x_set, y_set\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    def __len__(self):\n",
    "        return int(np.ceil(len(self.x) / float(self.batch_size)))\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        batch_x = self.x[idx * self.batch_size:(idx + 1) * self.batch_size]\n",
    "        batch_y = self.y[idx * self.batch_size:(idx + 1) * self.batch_size]\n",
    "        return np.array(batch_x), np.array(batch_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "631ba2bb-b60d-4d85-bdc0-ec5357d621d2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/8\n",
      "1125/1125 [==============================] - 153s 129ms/step - loss: 10091.6211 - mae: 91.6281 - val_loss: 9618.8262 - val_mae: 90.3144\n",
      "Epoch 2/8\n",
      "1125/1125 [==============================] - 136s 121ms/step - loss: 9066.6523 - mae: 87.5026 - val_loss: 7880.3149 - val_mae: 81.5333\n",
      "Epoch 3/8\n",
      "1125/1125 [==============================] - 147s 130ms/step - loss: 7422.4194 - mae: 78.7177 - val_loss: 6500.7998 - val_mae: 73.6468\n",
      "Epoch 4/8\n",
      "1125/1125 [==============================] - 166s 147ms/step - loss: 5463.3628 - mae: 66.2166 - val_loss: 4588.1348 - val_mae: 60.3210\n",
      "Epoch 5/8\n",
      "1125/1125 [==============================] - 143s 127ms/step - loss: 3592.6882 - mae: 51.7386 - val_loss: 2751.9417 - val_mae: 44.2943\n",
      "Epoch 6/8\n",
      "1125/1125 [==============================] - 140s 125ms/step - loss: 2088.0750 - mae: 37.1669 - val_loss: 1309.9443 - val_mae: 27.7875\n",
      "Epoch 7/8\n",
      "1125/1125 [==============================] - 150s 134ms/step - loss: 1101.5157 - mae: 25.2647 - val_loss: 643.6977 - val_mae: 18.2651\n",
      "Epoch 8/8\n",
      "1125/1125 [==============================] - 148s 132ms/step - loss: 550.4512 - mae: 17.2699 - val_loss: 322.2524 - val_mae: 12.1176\n"
     ]
    }
   ],
   "source": [
    "# Define the model with regularization\n",
    "base_model = ResNet50(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
    "x = base_model.output\n",
    "x = Flatten()(x)\n",
    "x = Dense(256, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.01))(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Dense(256, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.01))(x)\n",
    "x = BatchNormalization()(x)\n",
    "predictions = Dense(18, activation='linear')(x)\n",
    "\n",
    "model = Model(inputs=base_model.input, outputs=predictions)\n",
    "\n",
    "# Unfreeze layers starting from a certain index (e.g., unfreeze layers from 100 onwards)\n",
    "for layer in model.layers[100:]:\n",
    "    layer.trainable = True\n",
    "\n",
    "# Compile the model\n",
    "initial_learning_rate = 0.0001\n",
    "optimizer = Adam(learning_rate=initial_learning_rate)\n",
    "model.compile(optimizer=optimizer, loss='mean_squared_error', metrics=['mae'])\n",
    "\n",
    "# Early Stopping Callback\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
    "\n",
    "# Train the model with data generator\n",
    "batch_size = 8\n",
    "train_gen = DataGenerator(x_train, y_train, batch_size)\n",
    "test_gen = DataGenerator(x_test, y_test, batch_size)\n",
    "\n",
    "# Train the model with data generator\n",
    "history = model.fit(\n",
    "    train_gen,\n",
    "    epochs=15,\n",
    "    validation_data=test_gen,\n",
    "    callbacks=[early_stopping]\n",
    ")\n",
    "# Function to plot the training history\n",
    "def plot_training_history(history):\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(history.history['loss'], label='Training Loss')\n",
    "    plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(history.history['mae'], label='Training MAE')\n",
    "    plt.plot(history.history['val_mae'], label='Validation MAE')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Mean Absolute Error (MAE)')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Plot the training history\n",
    "plot_training_history(history)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0779c61-8ca8-4b88-82f7-548acfc848e1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4f0dd1c5-5eb1-46f4-8bc5-a1b0f920e6be",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 322.2524108886719\n",
      "Test MAE: 12.117645263671875\n",
      "125/125 [==============================] - 3s 23ms/step\n",
      "Example 1\n",
      "Predicted: [133.43936  104.63026  150.70647  116.362434 135.73407  128.2163\n",
      " 120.09697   78.546005 125.88072   32.86334  139.28737   69.43165\n",
      " 152.45874   82.1323   162.68492   69.45744  153.4139   107.99866 ]\n",
      "Actual: [135 103 160 114 132 135 133  83 133  38 148  73 175  83 206  62 196 112]\n",
      "\n",
      "Example 2\n",
      "Predicted: [107.56317   49.60391  120.7434    51.21089  112.84858   66.11668\n",
      "  94.52711   37.583775  91.001366  10.084259 106.75842   24.801477\n",
      " 116.54228   25.536345 123.237625   9.893718 117.179405  42.12436 ]\n",
      "Actual: [127  42 154  43 150  58  87  35  89   6 116  23 144  25 166  14 162  40]\n",
      "\n",
      "Example 3\n",
      "Predicted: [ 97.02936  122.32153  129.20622  139.51907  104.99069  152.28407\n",
      "  74.083565  85.85626   85.89133   17.36649  108.263435  72.05338\n",
      " 136.86436   92.923546 154.3864    77.44414  141.52005  128.74565 ]\n",
      "Actual: [ 83 129 137 163  91 191  65  80  93   9 114  75 157 102 213  76 183 156]\n",
      "\n",
      "Example 4\n",
      "Predicted: [75.920334 77.266685 85.15856  89.36055  74.076096 86.4488   72.06324\n",
      " 62.337704 79.663765 32.984383 85.19965  58.841515 89.75293  72.961266\n",
      " 93.43303  70.32598  88.44506  88.321945]\n",
      "Actual: [ 72  79  95  96  67 105  67  63  89  35  91  66 104  77 137  70 115  98]\n",
      "\n",
      "Example 5\n",
      "Predicted: [115.88333  108.29352  129.51894  110.892204 119.76312  112.46921\n",
      " 103.64642   99.07908  100.26948   77.29758  114.933334  92.48175\n",
      " 125.131874  94.10536  126.342705  83.96275  125.23885  104.38112 ]\n",
      "Actual: [122 107 140 107 130 116 111 101 105  89 124  97 141  96 158  87 157 101]\n",
      "\n",
      "Mean Absolute Error (MAE): 12.117645319114128\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model on the test set\n",
    "evaluation = model.evaluate(test_gen, verbose=0)\n",
    "print(\"Test Loss:\", evaluation[0])\n",
    "print(\"Test MAE:\", evaluation[1])\n",
    "\n",
    "# Make predictions using the trained model\n",
    "predictions = model.predict(test_gen)\n",
    "\n",
    "# Calculate and print some example predictions and actual values\n",
    "for i in range(5):\n",
    "    print(\"Example\", i+1)\n",
    "    print(\"Predicted:\", predictions[i])\n",
    "    print(\"Actual:\", y_test[i])\n",
    "    print(\"\")\n",
    "\n",
    "# Calculate the mean absolute error (MAE) of the predictions\n",
    "mae = np.mean(np.abs(predictions - y_test))\n",
    "print(\"Mean Absolute Error (MAE):\", mae)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e1ca30c-de99-4784-83f1-398c0107026f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c87ea26-89be-4905-ab4d-c47af347f6e1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb623c1f-69be-4996-bd03-c1f58655569a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10 (tensorflow-gpu)",
   "language": "python",
   "name": "tensorflow-gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
