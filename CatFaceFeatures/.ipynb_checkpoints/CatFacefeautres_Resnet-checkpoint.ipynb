{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cbdf08da",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Flatten, Dense\n",
    "from tensorflow.keras.utils import Sequence\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.applications.resnet50 import ResNet50\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a3b27945",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Avoid OOM errors by setting GPU Memory Consumption Growth\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "for gpu in gpus: \n",
    "    tf.config.experimental.set_memory_growth(gpu, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37aef5d9",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "656fb2e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.config.list_physical_devices('GPU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ea18c53d-9e35-48a1-add9-7d0e22a5407f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Function to read annotation data from the .cat files\n",
    "# This `read_annotation_file` function is responsible for reading the contents of the annotation files with the extension `.cat`. The annotations represent the location of certain points in the images, which are used for training a facial landmark detection model.\n",
    "# Let's go through the function step-by-step:\n",
    "# 1. `with open(annotation_file_path, 'r') as file:`: This line opens the annotation file in read mode (`'r'`) using a context manager (`with` statement). Using a context manager ensures that the file is properly closed after the reading is done, even if an exception occurs.\n",
    "# 2. `line = file.readline().strip()`: This reads the first line of the annotation file using `readline()`. The `strip()` method removes any leading and trailing whitespace characters (such as newline characters) from the line.\n",
    "# 3. `values = line.split()`: The line is split into a list of values using the `split()` method. By default, `split()` separates the values based on whitespace characters, which is what we need here.\n",
    "# 4. `num_points = int(values[0])`: The first value in the `values` list represents the number of points in the annotation. This value is converted to an integer using `int()` and assigned to the variable `num_points`.\n",
    "# 5. `annotation_data = [int(value) for value in values[1:]]`: The rest of the values in the `values` list (excluding the first value) represent the x and y coordinates of the points. The list comprehension `[int(value) for value in values[1:]]` iterates over these values, converts each value to an integer using `int()`, and creates a new list called `annotation_data`.\n",
    "# 6. `return num_points, annotation_data`: Finally, the function returns a tuple containing `num_points` and `annotation_data`. The `num_points` represents the number of points in the annotation, and `annotation_data` is a list containing the x and y coordinates of those points.\n",
    "# For example, given an annotation file with the following content:\n",
    "# 9 175 160 239 162 199 199 149 121 137 78 166 93 281 101 312 96 296 133\n",
    "# The function will read this line, extract the number of points (which is 9 in this case), and store the x and y coordinates of the 9 points in the `annotation_data` list. The function will then return the tuple `(9, [175, 160, 239, 162, 199, 199, 149, 121, 137, 78, 166, 93, 281, 101, 312, 96, 296, 133])`.\n",
    "def read_annotation_file(annotation_file_path):\n",
    "    with open(annotation_file_path, 'r') as file:\n",
    "        line = file.readline().strip()\n",
    "        values = line.split()\n",
    "        num_points = int(values[0])\n",
    "        annotation_data = [int(value) for value in values[1:]]\n",
    "        return num_points, annotation_data\n",
    "\n",
    "# Lists to store image data and annotation data\n",
    "images_and_annotations = []\n",
    "\n",
    "# The code you provided is used to collect the paths of image files (with extensions .jpg, .jpeg, and .png) and annotation files (with extension .cat) from a specified directory and its subdirectories. Let's break down the code step-by-step:\n",
    "# 1. `folder_path = r'C:\\Users\\haris\\test-installation\\Data\\Face Mask\\Cats'`: This line defines the directory path where the images and corresponding .cat files are located.\n",
    "# 2. `image_files_path = []`: This creates an empty list to store the paths of image files.\n",
    "# 3. `annotation_files_path = []`: This creates another empty list to store the paths of annotation files.\n",
    "# 4. `for root, dirs, files in os.walk(folder_path):`: The `os.walk()` function is used to traverse the specified directory and its subdirectories. It returns a generator that yields a tuple for each directory it encounters. The tuple contains three values:\n",
    "#    - `root`: The current directory being visited.\n",
    "#    - `dirs`: A list of subdirectories in the current directory.\n",
    "#    - `files`: A list of files in the current directory.\n",
    "# 5. `for file in files:`: This loop iterates over each file in the current directory.\n",
    "# 6. `if file.lower().endswith(('.jpg', '.jpeg', 'png')):`: This condition checks if the file has an image extension (.jpg, .jpeg, or .png). The `file.lower()` converts the filename to lowercase, and `endswith()` is used to check if the filename ends with any of the specified image extensions.\n",
    "# 7. `image_path = os.path.join(root, file)`: This line creates the full path to the image file by joining the current directory (`root`) with the filename (`file`) using `os.path.join()`.\n",
    "# 8. `image_files_path.append(image_path)`: The full image path is added to the `image_files_path` list.\n",
    "# 9. `if file.lower().endswith('.cat'):`: This condition checks if the file has a .cat extension.\n",
    "# 10. `annotation_path = os.path.join(root, file)`: Similar to step 7, this line creates the full path to the annotation file.\n",
    "# 11. `annotation_files_path.append(annotation_path)`: The full annotation file path is added to the `annotation_files_path` list.\n",
    "\n",
    "# Path to the directory containing the images and .cat files\n",
    "folder_path = r'C:\\Users\\haris\\test-installation\\Data\\Face Mask\\Cats'\n",
    "\n",
    "# Lists to store paths of .jpg files and .cat files\n",
    "image_files_path = []\n",
    "annotation_files_path = []\n",
    "\n",
    "for root, dirs, files in os.walk(folder_path):\n",
    "    # Loop through all the files in the folder\n",
    "    for file in files:\n",
    "        # Check if the file has a .jpg extension\n",
    "        if file.lower().endswith(('.jpg', '.jpeg', 'png')):\n",
    "            image_path = os.path.join(root, file)\n",
    "            image_files_path.append(image_path)\n",
    "\n",
    "        # Check if the file has a .cat extension\n",
    "        if file.lower().endswith('.cat'):\n",
    "            annotation_path = os.path.join(root, file)\n",
    "            annotation_files_path.append(annotation_path)\n",
    "            \n",
    "# After executing this code, `image_files_path` will contain a list of all the paths to the image files in\n",
    "# the specified directory and its subdirectories, and `annotation_files_path` will contain a list of all\n",
    "# the paths to the .cat annotation files corresponding to those images.\n",
    "image_size = (224, 224)\n",
    "# Read annotation data for each image\n",
    "for annotation_file_path in annotation_files_path:\n",
    "    num_points, annotation_data = read_annotation_file(annotation_file_path)\n",
    "\n",
    "    # Assuming images are stored in RGB format\n",
    "    image = cv2.imread(annotation_file_path[:-4], cv2.IMREAD_COLOR)\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)  # Convert BGR to RGB\n",
    "    # Resize images to a common size (e.g., 224x224)\n",
    "    resized_image = cv2.resize(image, image_size)\n",
    "\n",
    "    # # Calculate the scaling factor for annotation coordinates\n",
    "    scale_y = resized_image.shape[0] / image.shape[0]\n",
    "    scale_x = resized_image.shape[1] / image.shape[1]\n",
    "\n",
    "    # Apply scaling to the annotation data for both x and y coordinates together\n",
    "    resized_annotation_data = [int(value * scale_x) if i % 2 == 0 else int(value * scale_y) for i, value in enumerate(annotation_data)]\n",
    "    images_and_annotations.append((resized_image, resized_annotation_data))\n",
    "# Lists to store resized image data and annotation data\n",
    "resized_images = []\n",
    "resized_annotations = []\n",
    "\n",
    "# Iterate through the images_and_annotations list\n",
    "for resized_image, resized_annotation_data in images_and_annotations:\n",
    "    resized_images.append(resized_image)\n",
    "    resized_annotations.append(resized_annotation_data)\n",
    "\n",
    "# Convert lists to numpy arrays\n",
    "resized_images = np.array(resized_images)\n",
    "resized_annotations = np.array(resized_annotations)\n",
    "\n",
    "# Let's take a look at examples of resized images and their corresponding annotation data after the code block:\n",
    "# Assuming `images_and_annotations` contains the following data:\n",
    "# images_and_annotations = [\n",
    "#     (image1, annotation_data1),\n",
    "#     (image2, annotation_data2),\n",
    "# After resizing and converting to NumPy arrays, `images` will contain resized images, and `annotations` will contain corresponding annotation data.\n",
    "# Example of `images` array (resized images):\n",
    "# images = np.array([\n",
    "#     [[pixel1, pixel2, ...],  # Row 1 of Image 1\n",
    "#      [pixel3, pixel4, ...],  # Row 2 of Image 1\n",
    "#      [pixelN, pixelN+1, ...]],  # Last Row of Image 1\n",
    "#     [[pixel1, pixel2, ...],  # Row 1 of Image 2\n",
    "#      [pixel3, pixel4, ...],  # Row 2 of Image 2\n",
    "#      ...\n",
    "#      [pixelN, pixelN+1, ...]],  # Last Row of Image 2\n",
    "# Example of `annotations` array:\n",
    "# ```\n",
    "# annotations = np.array([\n",
    "#     [point1_x, point1_y, point2_x, point2_y, ...],  # Annotation data for Image 1\n",
    "#     [point1_x, point1_y, point2_x, point2_y, ...],  # Annotation data for Image 2\n",
    "# ```\n",
    "# Each element of the `images` array is a matrix representing a resized image. Each element of the `annotations` array is an array containing the annotation data for the corresponding image. The annotation data includes the x and y coordinates of points representing various facial features like eyes, ears, and mouth.\n",
    "# Keep in mind that the actual values of the pixel intensities and annotation coordinates are not shown here, as they can vary based on the specific images and annotation data present in the `images_and_annotations` list. The arrays `images` and `annotations` will contain numerical values representing the pixel intensities and coordinates, respectively. These arrays can be used for further analysis, processing, or training a machine learning model.\n",
    "\n",
    "# # Normalize annotation data (optional)\n",
    "# annotations = annotations / image_size[0]  # Assuming square images\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "x_train, x_test, y_train, y_test = train_test_split(resized_images, resized_annotations, test_size=0.1, random_state=42)\n",
    "\n",
    "class DataGenerator(Sequence):\n",
    "    def __init__(self, x_set, y_set, batch_size):\n",
    "        self.x, self.y = x_set, y_set\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    def __len__(self):\n",
    "        return int(np.ceil(len(self.x) / float(self.batch_size)))\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        batch_x = self.x[idx * self.batch_size:(idx + 1) * self.batch_size]\n",
    "        batch_y = self.y[idx * self.batch_size:(idx + 1) * self.batch_size]\n",
    "        return batch_x, batch_y\n",
    "\n",
    "train_gen = DataGenerator(x_train, y_train, 4)\n",
    "test_gen = DataGenerator(x_test, y_test, 4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2d1cb449-174f-48a4-9001-546274626c02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/6\n",
      "2250/2250 [==============================] - 1295s 572ms/step - loss: 967.8835 - val_loss: 871.2816\n",
      "Epoch 2/6\n",
      "2250/2250 [==============================] - 1248s 554ms/step - loss: 573.6827 - val_loss: 527.6223\n",
      "Epoch 3/6\n",
      "2250/2250 [==============================] - 1245s 553ms/step - loss: 395.5585 - val_loss: 472.5278\n",
      "Epoch 4/6\n",
      "2250/2250 [==============================] - 1239s 550ms/step - loss: 310.3137 - val_loss: 357.3329\n",
      "Epoch 5/6\n",
      "2250/2250 [==============================] - 1237s 550ms/step - loss: 251.3413 - val_loss: 396.4106\n",
      "Epoch 6/6\n",
      "2250/2250 [==============================] - 1163s 517ms/step - loss: 194.7454 - val_loss: 310.6384\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1f6412d61d0>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load pre-trained ResNet50 model\n",
    "base_model = ResNet50(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
    "\n",
    "# Add custom layers on top of the pre-trained model\n",
    "x = Flatten()(base_model.output)\n",
    "x = Dense(256, activation='relu')(x)\n",
    "x = Dense(256, activation='relu')(x)\n",
    "predictions = Dense(18, activation='linear')(x)  # Output layer with 18 units for facial landmark points\n",
    "\n",
    "# Create the model\n",
    "model = Model(inputs=base_model.input, outputs=predictions)\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "\n",
    "# Train the model\n",
    "model.fit(train_gen, epochs=6, validation_data=test_gen)\n",
    "\n",
    "# model.fit(x_train, y_train, epochs=10, batch_size=8, validation_data=(x_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4aa0d2be-1ea9-474a-b16b-f5aa9736a30e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Save the trained model\n",
    "model.save('CatFacefeautres_Resnet_model.h5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "17effe97-eeb7-4908-83c2-bd640325df80",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/6\n",
      "2250/2250 [==============================] - 577s 253ms/step - loss: 179.7864 - mae: 8.8660 - val_loss: 357.5716 - val_mae: 12.2331\n",
      "Epoch 2/6\n",
      "2250/2250 [==============================] - 569s 253ms/step - loss: 136.1289 - mae: 7.8110 - val_loss: 496.6866 - val_mae: 14.0447\n",
      "Epoch 3/6\n",
      "2250/2250 [==============================] - 567s 252ms/step - loss: 116.1500 - mae: 7.2887 - val_loss: 224.9633 - val_mae: 9.2348\n",
      "Epoch 4/6\n",
      "2250/2250 [==============================] - 567s 252ms/step - loss: 89.6767 - mae: 6.5366 - val_loss: 188.3518 - val_mae: 8.3772\n",
      "Epoch 5/6\n",
      "2250/2250 [==============================] - 574s 255ms/step - loss: 83.8503 - mae: 6.3072 - val_loss: 254.3738 - val_mae: 10.0268\n",
      "Epoch 6/6\n",
      "2250/2250 [==============================] - 571s 254ms/step - loss: 72.2600 - mae: 5.9306 - val_loss: 245.4337 - val_mae: 9.1075\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# Load the previously saved model\n",
    "your_model_file = 'CatFacefeautres_Resnet_model.h5'\n",
    "model = load_model(your_model_file)\n",
    "\n",
    "# Continue training for additional epochs (epochs 7 to 12)\n",
    "additional_epochs = 6  # Number of additional epochs (7 to 12)\n",
    "\n",
    "# Compile the model with L2 regularization\n",
    "l2_regularizer = l2(0.01)  # Adjust the regularization strength as needed\n",
    "optimizer = Adam(learning_rate=0.001)  # You can change the learning rate as needed\n",
    "\n",
    "# Apply L2 regularization to the trainable weights of the model\n",
    "for layer in model.layers:\n",
    "    if hasattr(layer, 'kernel_regularizer'):\n",
    "        layer.kernel_regularizer = l2_regularizer\n",
    "\n",
    "model.compile(optimizer=optimizer, loss='mean_squared_error', metrics=['mae'])\n",
    "\n",
    "# Train the model for additional epochs\n",
    "history_additional = model.fit(train_gen, epochs=additional_epochs, validation_data=test_gen)\n",
    "\n",
    "# Save the model after training for additional epochs\n",
    "model.save('CatFacefeautres_Resnet_model_additional_epochs.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "152bbee4-03e9-4f93-b9be-ed74d512d609",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "5472f156-f3a2-4d13-85d9-8cfda7b61209",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 57ms/step\n",
      "Predicted Landmarks: [140, 97, 186, 99, 164, 140, 110, 67, 110, 9, 142, 47, 185, 49, 222, 16, 214, 75]\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "\n",
    "# Function to predict facial landmarks on new images\n",
    "def predict_landmarks(image_path):\n",
    "    # Load the image and preprocess it\n",
    "    image = cv2.imread(image_path)\n",
    "    image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)  # Convert to RGB before resizing\n",
    "    resized_image = cv2.resize(image_rgb, image_size)\n",
    "    input_image = np.expand_dims(resized_image, axis=0)\n",
    "\n",
    "    # Make predictions using the trained model\n",
    "    predictions = model.predict(input_image)\n",
    "\n",
    "    # Rescale the predictions to the original image size\n",
    "    scale_y = image.shape[0] / image_size[0]\n",
    "    scale_x = image.shape[1] / image_size[1]\n",
    "    resized_predictions = [int(value * scale_x) if i % 2 == 0 else int(value * scale_y) for i, value in enumerate(predictions[0])]\n",
    "\n",
    "    return image, resized_predictions\n",
    "\n",
    "# Example usage:\n",
    "new_image_path = r'C:\\Users\\haris\\test-installation\\Data\\Face Mask\\download.jpeg'\n",
    "original_image, landmarks = predict_landmarks(new_image_path)\n",
    "print(\"Predicted Landmarks:\", landmarks)\n",
    "\n",
    "# Draw circles (dots) on the original image at the predicted landmark locations\n",
    "for i in range(0, len(landmarks), 2):\n",
    "    x, y = landmarks[i], landmarks[i + 1]\n",
    "    color = (0, 0, 255)  # Red color for the dots, you can change it to any desired color\n",
    "    radius = 3  # Adjust the size of the dots as needed\n",
    "    thickness = -1  # Fill the circles (dots) to make them solid\n",
    "    cv2.circle(original_image, (x, y), radius, color, thickness)\n",
    "\n",
    "# Show the image with predicted landmarks\n",
    "cv2.imshow('Predicted Landmarks', original_image)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f05e50a9-bb53-4256-b6b3-eb46ad6c30b2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10 (tensorflow-gpu)",
   "language": "python",
   "name": "tensorflow-gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
