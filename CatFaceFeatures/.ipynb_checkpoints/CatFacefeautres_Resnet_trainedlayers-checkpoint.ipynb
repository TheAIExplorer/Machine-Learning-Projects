{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "00ce1a07-8c71-47b0-b559-cefb2a9cf88b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Flatten, Dense, BatchNormalization\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import LearningRateScheduler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.applications.resnet50 import ResNet50\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.utils import Sequence\n",
    "\n",
    "\n",
    "# Avoid OOM errors by setting GPU Memory Consumption Growth\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "for gpu in gpus:\n",
    "    tf.config.experimental.set_memory_growth(gpu, True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9f715d4a-c997-4141-8881-dd7a7bec444a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to read annotation data from the .cat files\n",
    "def read_annotation_file(annotation_file_path):\n",
    "    with open(annotation_file_path, 'r') as file:\n",
    "        line = file.readline().strip()\n",
    "        values = line.split()\n",
    "        num_points = int(values[0])\n",
    "        annotation_data = [int(value) for value in values[1:]]\n",
    "        return num_points, annotation_data\n",
    "\n",
    "# Path to the directory containing the images and .cat files\n",
    "folder_path = r'C:\\Users\\haris\\test-installation\\Data\\Face Mask\\Cats'\n",
    "\n",
    "# Lists to store paths of .jpg files and .cat files\n",
    "image_files_path = []\n",
    "annotation_files_path = []\n",
    "\n",
    "for root, dirs, files in os.walk(folder_path):\n",
    "    for file in files:\n",
    "        if file.lower().endswith(('.jpg', '.jpeg', 'png')):\n",
    "            image_path = os.path.join(root, file)\n",
    "            image_files_path.append(image_path)\n",
    "\n",
    "        if file.lower().endswith('.cat'):\n",
    "            annotation_path = os.path.join(root, file)\n",
    "            annotation_files_path.append(annotation_path)\n",
    "\n",
    "# Lists to store image data and annotation data\n",
    "images_and_annotations = []\n",
    "\n",
    "image_size = (224, 224)\n",
    "\n",
    "# Read annotation data for each image and apply normalization\n",
    "for annotation_file_path in annotation_files_path:\n",
    "    num_points, annotation_data = read_annotation_file(annotation_file_path)\n",
    "\n",
    "    image = cv2.imread(annotation_file_path[:-4], cv2.IMREAD_COLOR)\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    resized_image = cv2.resize(image, image_size)\n",
    "    \n",
    "    # # Calculate the scaling factor for annotation coordinates\n",
    "    scale_y = resized_image.shape[0] / image.shape[0]\n",
    "    scale_x = resized_image.shape[1] / image.shape[1]\n",
    "\n",
    "    # Apply scaling to the annotation data for both x and y coordinates together\n",
    "    resized_annotation_data = [int(value * scale_x) if i % 2 == 0 else int(value * scale_y) for i, value in enumerate(annotation_data)]\n",
    "    images_and_annotations.append((resized_image, resized_annotation_data))\n",
    "\n",
    "# Lists to store resized image data and annotation data\n",
    "resized_images = []\n",
    "resized_annotations = []\n",
    "\n",
    "# Iterate through the images_and_annotations list\n",
    "for resized_image, resized_annotation_data in images_and_annotations:\n",
    "    resized_images.append(resized_image)\n",
    "    resized_annotations.append(resized_annotation_data)\n",
    "\n",
    "# Convert lists to numpy arrays\n",
    "resized_images = np.array(resized_images)\n",
    "resized_annotations = np.array(resized_annotations)\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "60260171-2af7-4db0-81de-93d06905b257",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and testing sets\n",
    "x_train, x_test, y_train, y_test = train_test_split(resized_images, resized_annotations, test_size=0.1, random_state=42)\n",
    "\n",
    "class DataGenerator(Sequence):\n",
    "    def __init__(self, x_set, y_set, batch_size):\n",
    "        self.x, self.y = x_set, y_set\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    def __len__(self):\n",
    "        return int(np.ceil(len(self.x) / float(self.batch_size)))\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        batch_x = self.x[idx * self.batch_size:(idx + 1) * self.batch_size]\n",
    "        batch_y = self.y[idx * self.batch_size:(idx + 1) * self.batch_size]\n",
    "        return batch_x, batch_y\n",
    "\n",
    "train_gen = DataGenerator(x_train, y_train, batch_size=32)\n",
    "test_gen = DataGenerator(x_test, y_test, batch_size=32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ccf99003-1399-4a9a-a148-e9f232e5cd7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/12\n",
      "282/282 [==============================] - 39s 102ms/step - loss: 8248.1553 - mae: 82.7411 - val_loss: 5074.3579 - val_mae: 64.9837\n",
      "Epoch 2/12\n",
      "282/282 [==============================] - 27s 97ms/step - loss: 1713.5146 - mae: 31.3738 - val_loss: 524.8253 - val_mae: 15.9933\n",
      "Epoch 3/12\n",
      "282/282 [==============================] - 28s 98ms/step - loss: 226.9642 - mae: 9.6736 - val_loss: 252.4578 - val_mae: 9.3136\n",
      "Epoch 4/12\n",
      "282/282 [==============================] - 28s 98ms/step - loss: 160.8627 - mae: 7.6660 - val_loss: 234.1105 - val_mae: 8.7239\n",
      "Epoch 5/12\n",
      "282/282 [==============================] - 28s 98ms/step - loss: 148.6369 - mae: 7.2574 - val_loss: 260.2658 - val_mae: 9.3822\n",
      "Epoch 6/12\n",
      "282/282 [==============================] - 28s 98ms/step - loss: 144.2939 - mae: 7.1201 - val_loss: 246.8748 - val_mae: 9.2226\n",
      "Epoch 7/12\n",
      "282/282 [==============================] - 28s 98ms/step - loss: 142.9897 - mae: 7.0495 - val_loss: 225.4354 - val_mae: 8.3111\n",
      "Epoch 8/12\n",
      "282/282 [==============================] - 28s 98ms/step - loss: 139.1432 - mae: 6.9296 - val_loss: 250.7306 - val_mae: 9.4975\n",
      "Epoch 9/12\n",
      "282/282 [==============================] - 28s 98ms/step - loss: 133.5108 - mae: 6.7685 - val_loss: 228.1612 - val_mae: 8.3826\n",
      "Epoch 10/12\n",
      "282/282 [==============================] - 28s 99ms/step - loss: 131.4225 - mae: 6.6736 - val_loss: 217.8890 - val_mae: 8.1208\n",
      "Epoch 11/12\n",
      "282/282 [==============================] - 28s 98ms/step - loss: 128.4880 - mae: 6.6307 - val_loss: 224.6992 - val_mae: 8.4486\n",
      "Epoch 12/12\n",
      "282/282 [==============================] - 28s 98ms/step - loss: 126.5127 - mae: 6.5721 - val_loss: 318.0491 - val_mae: 11.1522\n"
     ]
    }
   ],
   "source": [
    "# Define the model with regularization\n",
    "base_model = ResNet50(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
    "x = base_model.output\n",
    "x = Flatten()(x)\n",
    "x = Dense(256, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.01))(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Dense(256, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.01))(x)\n",
    "x = BatchNormalization()(x)\n",
    "predictions = Dense(18, activation='linear')(x)\n",
    "\n",
    "model = Model(inputs=base_model.input, outputs=predictions)\n",
    "\n",
    "# Freeze the layers\n",
    "for layer in base_model.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "# Compile the model\n",
    "initial_learning_rate = 0.001\n",
    "optimizer = Adam(learning_rate=initial_learning_rate)\n",
    "model.compile(optimizer=optimizer, loss='mean_squared_error', metrics=['mae'])\n",
    "\n",
    "# Early Stopping Callback\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
    "\n",
    "# Train the model with data augmentation\n",
    "history = model.fit(\n",
    "    train_gen,\n",
    "    epochs=12,\n",
    "    validation_data=test_gen,\n",
    "    callbacks=[early_stopping]\n",
    ")\n",
    "# Save the trained model\n",
    "model.save('CatFaceFeatures_Resnet_model_trainedlayers.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdc36c97-d2f0-44d2-a1d0-280a50d7a2db",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 26ms/step\n",
      "Predicted Landmarks: [73, 104, 125, 102, 95, 154, 45, 80, 36, 6, 75, 44, 127, 44, 169, 2, 164, 79]\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "\n",
    "# Function to predict facial landmarks on new images\n",
    "def predict_landmarks(image_path):\n",
    "    # Load the image and preprocess it\n",
    "    image = cv2.imread(image_path)\n",
    "    image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)  # Convert to RGB before resizing\n",
    "    resized_image = cv2.resize(image_rgb, image_size)\n",
    "    input_image = np.expand_dims(resized_image, axis=0)\n",
    "\n",
    "    # Make predictions using the trained model\n",
    "    predictions = model.predict(input_image)\n",
    "\n",
    "    # Rescale the predictions to the original image size\n",
    "    scale_y = image.shape[0] / image_size[0]\n",
    "    scale_x = image.shape[1] / image_size[1]\n",
    "    resized_predictions = [int(value * scale_x) if i % 2 == 0 else int(value * scale_y) for i, value in enumerate(predictions[0])]\n",
    "\n",
    "    return image, resized_predictions\n",
    "\n",
    "# Example usage:\n",
    "new_image_path = r'C:\\Users\\haris\\test-installation\\Data\\Face Mask\\download.jpeg'\n",
    "original_image, landmarks = predict_landmarks(new_image_path)\n",
    "print(\"Predicted Landmarks:\", landmarks)\n",
    "\n",
    "# Draw circles (dots) on the original image at the predicted landmark locations\n",
    "for i in range(0, len(landmarks), 2):\n",
    "    x, y = landmarks[i], landmarks[i + 1]\n",
    "    color = (0, 0, 255)  # Red color for the dots, you can change it to any desired color\n",
    "    radius = 3  # Adjust the size of the dots as needed\n",
    "    thickness = -1  # Fill the circles (dots) to make them solid\n",
    "    cv2.circle(original_image, (x, y), radius, color, thickness)\n",
    "\n",
    "# Show the image with predicted landmarks\n",
    "cv2.imshow('Predicted Landmarks', original_image)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d82343c-655e-4c46-abd4-f92e84220ccd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10 (tensorflow-gpu)",
   "language": "python",
   "name": "tensorflow-gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
