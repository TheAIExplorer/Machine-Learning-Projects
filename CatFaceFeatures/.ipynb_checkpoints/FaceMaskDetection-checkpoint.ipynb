{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "06e69081-6364-4110-bc63-93a4fb7b79d2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Import the required libraries.\n",
    "import os\n",
    "import cv2\n",
    "import albumentations as A\n",
    "import xml.etree.ElementTree as ET\n",
    "from tensorflow.keras.applications.vgg16 import VGG16\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "179fc893-927d-427b-b78e-64a5b7593222",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define the paths to the images and annotations folders\n",
    "images_folder = 'C:/Users/haris/test-installation/Data/Face Mask/images'\n",
    "annotations_folder = 'C:/Users/haris/test-installation/Data/Face Mask/annotations'\n",
    "# output_images_folder = 'C:/Users/haris/test-installation/Data/Face Mask/images'\n",
    "# output_annotations_folder = 'C:/Users/haris/test-installation/Data/Face Mask/annotations'\n",
    "\n",
    "# Create the output folders if they don't exist\n",
    "# os.makedirs(output_images_folder, exist_ok=True)\n",
    "# os.makedirs(output_annotations_folder, exist_ok=True)\n",
    "\n",
    "# List all image file names in the folder\n",
    "image_files = os.listdir(images_folder)\n",
    "\n",
    "# Define the augmentation pipeline\n",
    "transform = A.Compose([\n",
    "    # A.RandomCrop(width=256, height=256),\n",
    "    A.HorizontalFlip(p=0.5),\n",
    "    A.RandomBrightnessContrast(p=0.2),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0ea56a1b-4805-4f1f-9604-8fe82ad86837",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Loop through each image file\n",
    "for image_file in image_files:\n",
    "    # Read the image\n",
    "    image_path = os.path.join(images_folder, image_file)\n",
    "    image = cv2.imread(image_path)\n",
    "\n",
    "    # Read the corresponding annotation XML file\n",
    "    annotation_file = os.path.splitext(image_file)[0] + '.xml'\n",
    "# os.path.splitext(image_file): This function splits the image_file into its base filename and extension. For example, if image_file is 'example.jpg', the function will return ('example', '.jpg').\n",
    "# [0]: The [0] indexing is used to access the first element of the returned tuple, which is the base filename. In the example above, it will return 'example'.\n",
    "# + '.xml': The + operator concatenates the base filename obtained in the previous step with the string '.xml'. This is done to create the filename of the corresponding XML annotation file. For example, if the base filename is 'example', the resulting annotation_file will be 'example.xml'.\n",
    "# So, the line of code combines the base filename of an image file with the extension .xml to generate the corresponding filename for the XML annotation file associated with that image.\n",
    "    # Read the corresponding annotation XML file\n",
    "    annotation_path = os.path.join(annotations_folder, annotation_file)\n",
    "    with open(annotation_path, 'r') as f:\n",
    "        annotation_content = f.read()\n",
    "\n",
    "    # Apply the augmentation pipeline to the image and annotation\n",
    "    augmented = transform(image=image, xml=annotation_content)\n",
    "\n",
    "    # Get the augmented image and annotation\n",
    "    augmented_image = augmented['image']\n",
    "    augmented_annotation = augmented['xml']\n",
    "\n",
    "    # Save the augmented image\n",
    "    output_image_file = os.path.splitext(image_file)[0] + 'A.png'\n",
    "    output_image_path = os.path.join(images_folder, output_image_file)\n",
    "    cv2.imwrite(output_image_path, augmented_image)\n",
    "\n",
    "    # Save the augmented annotation\n",
    "    output_annotation_file = os.path.splitext(image_file)[0] + 'A.xml'\n",
    "    output_annotation_path = os.path.join(annotations_folder, output_annotation_file)\n",
    "    with open(output_annotation_path, 'w') as f:\n",
    "        f.write(augmented_annotation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0f42ac95-77da-4ecc-80b2-8f53968e5500",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 0 images belonging to 0 classes.\n",
      "Found 0 images belonging to 0 classes.\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'val_generator' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[24], line 93\u001b[0m\n\u001b[0;32m     85\u001b[0m         train_labels_encoded[i, label_index] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m     88\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[0;32m     89\u001b[0m model\u001b[38;5;241m.\u001b[39mfit(\n\u001b[0;32m     90\u001b[0m     train_generator,\n\u001b[0;32m     91\u001b[0m     steps_per_epoch\u001b[38;5;241m=\u001b[39mtrain_generator\u001b[38;5;241m.\u001b[39mn \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m batch_size,\n\u001b[0;32m     92\u001b[0m     epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m,\n\u001b[1;32m---> 93\u001b[0m     validation_data\u001b[38;5;241m=\u001b[39m\u001b[43mval_generator\u001b[49m,\n\u001b[0;32m     94\u001b[0m     validation_steps\u001b[38;5;241m=\u001b[39mval_generator\u001b[38;5;241m.\u001b[39mn \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m batch_size)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'val_generator' is not defined"
     ]
    }
   ],
   "source": [
    "# Load the VGG16 model without the top layers\n",
    "base_model = VGG16(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
    "\n",
    "# Freeze the pre-trained layers in the base model\n",
    "for layer in base_model.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "# Add custom fully connected layers on top of the base model\n",
    "x = base_model.output\n",
    "x = GlobalAveragePooling2D()(x)\n",
    "x = Dense(256, activation='relu')(x)\n",
    "predictions = Dense(2, activation='softmax')(x)\n",
    "\n",
    "# Create the final model by combining the base model and custom layers\n",
    "model = Model(inputs=base_model.input, outputs=predictions)\n",
    "\n",
    "# Compile the model with an appropriate optimizer and loss function\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Set the batch size and image dimensions\n",
    "batch_size = 32\n",
    "image_width, image_height = 224, 224\n",
    "\n",
    "# Create an ImageDataGenerator for training data\n",
    "train_datagen = ImageDataGenerator(rescale=1./255)\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    images_folder,\n",
    "    target_size=(image_width, image_height),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical')\n",
    "\n",
    "# Create an ImageDataGenerator for augmented data\n",
    "augmented_datagen = ImageDataGenerator(rescale=1./255)\n",
    "augmented_generator = augmented_datagen.flow_from_directory(\n",
    "    output_images_folder,\n",
    "    target_size=(image_width, image_height),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical')\n",
    "\n",
    "# Create a function to extract annotations from XML files\n",
    "def extract_annotations(xml_file):\n",
    "    tree = ET.parse(xml_file)\n",
    "    root = tree.getroot()\n",
    "    annotations = []\n",
    "    for obj in root.findall('object'):\n",
    "        name = obj.find('name').text\n",
    "        annotations.append(name)\n",
    "    return annotations\n",
    "\n",
    "# Generate training data by combining original and augmented images\n",
    "train_data = []\n",
    "train_labels = []\n",
    "for image_file in train_generator.filenames:\n",
    "    image_path = os.path.join(images_folder, image_file)\n",
    "    annotation_file = os.path.splitext(image_file)[0] + '.xml'\n",
    "    annotation_path = os.path.join(annotations_folder, annotation_file)\n",
    "    \n",
    "    annotations = extract_annotations(annotation_path)\n",
    "    \n",
    "    image = cv2.imread(image_path)\n",
    "    train_data.append(image)\n",
    "    train_labels.append(annotations)\n",
    "\n",
    "for image_file in augmented_generator.filenames:\n",
    "    image_path = os.path.join(output_images_folder, image_file)\n",
    "    annotation_file = os.path.splitext(image_file)[0] + '_aug.xml'\n",
    "    annotation_path = os.path.join(output_annotations_folder, annotation_file)\n",
    "    \n",
    "    annotations = extract_annotations(annotation_path)\n",
    "    \n",
    "    image = cv2.imread(image_path)\n",
    "    train_data.append(image)\n",
    "    train_labels.append(annotations)\n",
    "num_classes = 3\n",
    "\n",
    "# Convert the training data and labels to numpy arrays\n",
    "train_data = np.array(train_data)\n",
    "train_labels = np.array(train_labels)\n",
    "\n",
    "# Perform one-hot encoding on the labels\n",
    "train_labels_encoded = np.zeros((len(train_labels), num_classes))\n",
    "for i, labels in enumerate(train_labels):\n",
    "    for label in labels:\n",
    "        label_index = class_mapping[label]\n",
    "        train_labels_encoded[i, label_index] = 1\n",
    "\n",
    "\n",
    "# Train the model\n",
    "model.fit(\n",
    "    train_generator,\n",
    "    steps_per_epoch=train_generator.n // batch_size,\n",
    "    epochs=10,\n",
    "    validation_data=val_generator,\n",
    "    validation_steps=val_generator.n // batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82c6020c-10c2-4cb1-baf9-38693fa60710",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5b10d21d-cec1-4a2c-abb8-8b380f32637e",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bf9621f8-24df-431f-b42b-d517c1674dbf",
   "metadata": {
    "tags": []
   },
   "source": [
    "<!-- Bounding boxes are rectangles that mark objects on an image.\n",
    "There are multiple formats of bounding boxes annotations.\n",
    "Each format uses its specific representation of bouning boxes coordinates.\n",
    "Albumentations supports four formats: pascal_voc, albumentations, coco, and yolo . -->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97f545b1-cf20-4cdd-aa2b-3bb6c519b05b",
   "metadata": {},
   "source": [
    "<!-- RandomSizedBBoxSafeCrop crops a random part of the image. It ensures that the cropped part will contain all bounding boxes from the original image. Then the transform rescales the crop to height and width specified by the respective parameters. The erosion_rate parameter controls how much area of the original bounding box could be lost after cropping. erosion_rate = 0.2 means that the augmented bounding box's area could be up to 20% smaller than the area of the original bounding box. -->\n",
    "<!-- Let's look at an example:\n",
    "transform = A.Compose([\n",
    "    A.RandomCrop(width=256, height=256),\n",
    "    A.HorizontalFlip(p=0.5),\n",
    "    A.RandomBrightnessContrast(p=0.2),\n",
    "])\n",
    "In the example, Compose receives a list with three augmentations: A.RandomCrop, A.HorizontalFlip, and A.RandomBrighntessContrast. You can find the full list of all available augmentations in the GitHub repository and in the API Docs. A demo playground that demonstrates how augmentations will transform the input image is available at https://demo.albumentations.ai.\n",
    "\n",
    "To create an augmentation, you create an instance of the required augmentation class and pass augmentation parameters to it. A.RandomCrop receives two parameters, height and width. A.RandomCrop(width=256, height=256) means that A.RandomCrop will take an input image, extract a random patch with size 256 by 256 pixels from it and then pass the result to the next augmentation in the pipeline (in this case to A.HorizontalFlip).\n",
    "\n",
    "A.HorizontalFlip in this example has one parameter named p. p is a special parameter that is supported by almost all augmentations. It controls the probability of applying the augmentation. p=0.5 means that with a probability of 50%, the transform will flip the image horizontally, and with a probability of 50%, the transform won't modify the input image.\n",
    "\n",
    "A.RandomBrighntessContrast in the example also has one parameter, p. With a probability of 20%, this augmentation will change the brightness and contrast of the image received from A.HorizontalFlip. And with a probability of 80%, it will keep the received image unchanged. --><!-- Let's look at an example:\n",
    "transform = A.Compose([\n",
    "    A.RandomCrop(width=256, height=256),\n",
    "    A.HorizontalFlip(p=0.5),\n",
    "    A.RandomBrightnessContrast(p=0.2),\n",
    "])\n",
    "In the example, Compose receives a list with three augmentations: A.RandomCrop, A.HorizontalFlip, and A.RandomBrighntessContrast. You can find the full list of all available augmentations in the GitHub repository and in the API Docs. A demo playground that demonstrates how augmentations will transform the input image is available at https://demo.albumentations.ai.\n",
    "\n",
    "To create an augmentation, you create an instance of the required augmentation class and pass augmentation parameters to it. A.RandomCrop receives two parameters, height and width. A.RandomCrop(width=256, height=256) means that A.RandomCrop will take an input image, extract a random patch with size 256 by 256 pixels from it and then pass the result to the next augmentation in the pipeline (in this case to A.HorizontalFlip).\n",
    "\n",
    "A.HorizontalFlip in this example has one parameter named p. p is a special parameter that is supported by almost all augmentations. It controls the probability of applying the augmentation. p=0.5 means that with a probability of 50%, the transform will flip the image horizontally, and with a probability of 50%, the transform won't modify the input image.\n",
    "\n",
    "A.RandomBrighntessContrast in the example also has one parameter, p. With a probability of 20%, this augmentation will change the brightness and contrast of the image received from A.HorizontalFlip. And with a probability of 80%, it will keep the received image unchanged. -->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76d739d5-f7a5-4779-83a1-7d2f4288f5e8",
   "metadata": {},
   "source": [
    "<!-- Bounding boxes can be stored on the disk in different serialization\n",
    "formats: JSON, XML, YAML, CSV, etc. So the code to read bounding boxes\n",
    "depends on the actual format of data on the disk.\n",
    "\n",
    "From the provided XML file, the following information can be inferred:\n",
    "\n",
    "Folder: The folder where the image file is located is \"images\".\n",
    "\n",
    "Filename: The name of the image file is \"maksssksksss3.png\".\n",
    "\n",
    "Size: The size of the image is specified with the following attributes:\n",
    "\n",
    "Width: 400 pixels\n",
    "Height: 271 pixels\n",
    "Depth: 3 channels (indicating a color image with RGB channels)\n",
    "Segmented: The segmented value is 0, indicating that the image is not segmented.\n",
    "\n",
    "Objects: There are multiple objects present in the image, each represented by an \"object\" tag. The objects have the following attributes:\n",
    "\n",
    "Name: All objects are labeled as \"with_mask\".\n",
    "Pose: The pose is specified as \"Unspecified\" for all objects.\n",
    "Truncated, Occluded, Difficult: These attributes are set to 0, indicating that the objects are not truncated, occluded, or considered difficult.\n",
    "Bounding Box (Bndbox): Each object has a bounding box defined by four coordinates:\n",
    "xmin: The minimum x-coordinate of the bounding box.\n",
    "ymin: The minimum y-coordinate of the bounding box.\n",
    "xmax: The maximum x-coordinate of the bounding box.\n",
    "ymax: The maximum y-coordinate of the bounding box.\n",
    "In summary, the XML file contains information about the image file, its size, the presence of objects, and their corresponding bounding boxes. This information is typically used for tasks like object detection or annotation in computer vision applications. -->"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10 (tensorflow-gpu)",
   "language": "python",
   "name": "tensorflow-gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
