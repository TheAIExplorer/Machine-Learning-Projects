{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e942858e-6f04-4c18-9eb7-4c765fdb59c7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "80/80 [==============================] - 18s 178ms/step - loss: 2.8514 - accuracy: 0.9645 - val_loss: 4.1043 - val_accuracy: 0.0000e+00\n",
      "Epoch 2/10\n",
      "80/80 [==============================] - 14s 175ms/step - loss: 0.2203 - accuracy: 0.9707 - val_loss: 57.3388 - val_accuracy: 0.0000e+00\n",
      "Epoch 3/10\n",
      "80/80 [==============================] - 14s 176ms/step - loss: 5.8500 - accuracy: 0.9746 - val_loss: 330.7182 - val_accuracy: 0.0000e+00\n",
      "Epoch 4/10\n",
      "80/80 [==============================] - 14s 177ms/step - loss: 0.6206 - accuracy: 0.9746 - val_loss: 180.5731 - val_accuracy: 0.0000e+00\n",
      "Epoch 5/10\n",
      "80/80 [==============================] - 14s 177ms/step - loss: 0.3323 - accuracy: 0.9734 - val_loss: 15.3327 - val_accuracy: 0.0000e+00\n",
      "Epoch 6/10\n",
      "80/80 [==============================] - 14s 178ms/step - loss: 1.2165 - accuracy: 0.9727 - val_loss: 1.8477 - val_accuracy: 0.0000e+00\n",
      "Epoch 7/10\n",
      "80/80 [==============================] - 14s 178ms/step - loss: 0.1987 - accuracy: 0.9750 - val_loss: 2.4106 - val_accuracy: 0.0000e+00\n",
      "Epoch 8/10\n",
      "80/80 [==============================] - 14s 179ms/step - loss: 0.1515 - accuracy: 0.9750 - val_loss: 2.7904 - val_accuracy: 0.0000e+00\n",
      "Epoch 9/10\n",
      "80/80 [==============================] - 14s 179ms/step - loss: 0.1331 - accuracy: 0.9750 - val_loss: 3.0664 - val_accuracy: 0.0000e+00\n",
      "Epoch 10/10\n",
      "80/80 [==============================] - 14s 180ms/step - loss: 0.1265 - accuracy: 0.9750 - val_loss: 3.2649 - val_accuracy: 0.0000e+00\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2af82a44c40>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "from tensorflow.keras.applications import VGG16\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Flatten, Dropout\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.model_selection import train_test_split\n",
    "from albumentations import Compose, HorizontalFlip, VerticalFlip, RandomRotate90, ShiftScaleRotate, Blur, RandomBrightnessContrast\n",
    "\n",
    "data_dir = r'C:\\Users\\haris\\ArtificialIntelligence\\MachineLearning\\Projects\\Angry Sad Happy'\n",
    "\n",
    "# Define the augmentation transformations\n",
    "augmentation_transform = Compose([\n",
    "    HorizontalFlip(p=0.5),\n",
    "    VerticalFlip(p=0.5),\n",
    "    RandomRotate90(p=0.5),\n",
    "    ShiftScaleRotate(shift_limit=0.1, scale_limit=0.2, rotate_limit=45, p=0.5),\n",
    "    Blur(p=0.2),\n",
    "    RandomBrightnessContrast(p=0.3),\n",
    "])\n",
    "# Function to load and augment images in batches\n",
    "def image_data_generator(batch_size=32):\n",
    "    while True:\n",
    "        batch_images = []\n",
    "        batch_labels = []\n",
    "        for emotion in ['Happy', 'Sad', 'Angry']:\n",
    "            folder_path = os.path.join(data_dir, emotion)\n",
    "            for filename in os.listdir(folder_path):\n",
    "                image_path = os.path.join(folder_path, filename)\n",
    "                image = cv2.imread(image_path)\n",
    "                image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "                image = cv2.resize(image, (224, 224))  # Resize images to VGG input size\n",
    "                for _ in range(batch_size):\n",
    "                    augmented = augmentation_transform(image=image)\n",
    "                    augmented_image = augmented['image']\n",
    "                    batch_images.append(augmented_image)\n",
    "                    batch_labels.append(emotion)\n",
    "                    if len(batch_images) == batch_size:\n",
    "                        yield np.array(batch_images), np.array(batch_labels)\n",
    "                        batch_images = []\n",
    "                        batch_labels = []\n",
    "\n",
    "# Create the data generator\n",
    "batch_size = 32\n",
    "data_generator = image_data_generator(batch_size=batch_size)\n",
    "\n",
    "# Create a list to store data\n",
    "images = []\n",
    "labels = []\n",
    "\n",
    "# Load and augment data using the generator\n",
    "num_samples_per_class = 100  # Increase the number of samples per class\n",
    "for _ in range(num_samples_per_class):\n",
    "    X_batch, y_batch = next(data_generator)\n",
    "    images.extend(X_batch)\n",
    "    labels.extend(y_batch)\n",
    "\n",
    "# Create a list to store training and validation data separately\n",
    "train_images = []\n",
    "train_labels = []\n",
    "val_images = []\n",
    "val_labels = []\n",
    "\n",
    "# Load and augment data using the generator and separate it into training and validation sets\n",
    "num_samples_per_class = 100\n",
    "num_train_samples = int(num_samples_per_class * 0.8)  # 80% for training, 20% for validation\n",
    "\n",
    "for _ in range(num_train_samples):\n",
    "    X_batch, y_batch = next(data_generator)\n",
    "    train_images.extend(X_batch)\n",
    "    train_labels.extend(y_batch)\n",
    "\n",
    "for _ in range(num_samples_per_class - num_train_samples):\n",
    "    X_batch, y_batch = next(data_generator)\n",
    "    val_images.extend(X_batch)\n",
    "    val_labels.extend(y_batch)\n",
    "\n",
    "# Convert the list of images and labels to numpy arrays\n",
    "X_train = np.array(train_images)\n",
    "y_train_labels = np.array(train_labels)\n",
    "X_val = np.array(val_images)\n",
    "y_val_labels = np.array(val_labels)\n",
    "\n",
    "# Encode labels into numeric form\n",
    "label_to_numeric = {'Happy': 0, 'Sad': 1, 'Angry': 2}\n",
    "y_train_numeric = np.array([label_to_numeric[label] for label in y_train_labels])\n",
    "y_val_numeric = np.array([label_to_numeric[label] for label in y_val_labels])\n",
    "\n",
    "# Convert labels to one-hot encoded format\n",
    "num_classes = 3\n",
    "y_train_one_hot = to_categorical(y_train_numeric, num_classes=num_classes)\n",
    "y_val_one_hot = to_categorical(y_val_numeric, num_classes=num_classes)\n",
    "\n",
    "# Load the pre-trained VGG16 model (without the top classification layers)\n",
    "vgg16 = VGG16(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
    "\n",
    "# Freeze the layers of the pre-trained VGG16 model except the last five layers\n",
    "for layer in vgg16.layers[:-5]:\n",
    "    layer.trainable = False\n",
    "\n",
    "# Create a new model by adding custom classification layers on top of the pre-trained VGG16\n",
    "model = Sequential([\n",
    "    vgg16,\n",
    "    Flatten(),\n",
    "    Dense(256, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(3, activation='softmax')\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train_one_hot, epochs=10, batch_size=32, validation_data=(X_val, y_val_one_hot))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0afd5226-09a8-44ae-97f7-5c949797ede1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 21ms/step\n",
      "Predicted emotion: Happy\n",
      "Predicted probabilities: [0. 1. 0.]\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.applications.vgg16 import VGG16, preprocess_input\n",
    "\n",
    "# Load an image for prediction\n",
    "image_path = r'C:\\Users\\haris\\ArtificialIntelligence\\MachineLearning\\Projects\\Angry Sad Happy\\5cd88fd321000035007f6cd2.jpeg'\n",
    "image = cv2.imread(image_path)\n",
    "image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "image = cv2.resize(image, (224, 224))  # Resize to VGG input size\n",
    "\n",
    "# Preprocess the image (use VGG16 preprocess_input)\n",
    "image = preprocess_input(image)\n",
    "\n",
    "# Expand the dimensions to match the input shape of the model\n",
    "image = np.expand_dims(image, axis=0)\n",
    "\n",
    "# Make the prediction\n",
    "predicted_probs = model.predict(image)\n",
    "\n",
    "# Get the predicted class (index with highest probability)\n",
    "predicted_class = np.argmax(predicted_probs)\n",
    "\n",
    "# Convert the numeric class to emotion label\n",
    "numeric_to_label = {0: 'Angry', 1: 'Happy', 2: 'Sad'}  # Update the mapping\n",
    "predicted_emotion = numeric_to_label[predicted_class]\n",
    "\n",
    "print(f\"Predicted emotion: {predicted_emotion}\")\n",
    "print(f\"Predicted probabilities: {predicted_probs[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbb23f7c-1881-41da-aa39-8d7f4d299d39",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10 (tensorflow-gpu)",
   "language": "python",
   "name": "tensorflow-gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
