{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "95706a33-f665-4087-adb3-cf514aae96b9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import cv2\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras import layers\n",
    "from functools import partial\n",
    "\n",
    "import os\n",
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, BatchNormalization, ReLU, MaxPooling2D, Dropout, Flatten, Dense\n",
    "from tensorflow.keras.applications import Xception\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "AUTO = tf.data.experimental.AUTOTUNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b1b185fa-ad76-4430-8380-da29c8fd4fdc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Path to the folders containing images and annotations\n",
    "annotations_folder = r'C:\\Users\\haris\\AI\\ML\\Projects\\Data\\Dog Breed Classification\\annotations'\n",
    "images_folder = r'C:\\Users\\haris\\AI\\ML\\Projects\\Data\\Dog Breed Classification\\images'\n",
    "\n",
    "# Initialize lists to store image data and labels\n",
    "images = []\n",
    "labels = []\n",
    "\n",
    "# Parse XML annotations and load images\n",
    "for breed_folder in os.listdir(annotations_folder):\n",
    "    breed_path = os.path.join(annotations_folder, breed_folder)\n",
    "    for annotation_file in os.listdir(breed_path):\n",
    "        annotation_path = os.path.join(breed_path, annotation_file)\n",
    "        \n",
    "        tree = ET.parse(annotation_path)\n",
    "        root = tree.getroot()\n",
    "        \n",
    "        breed = root.find('object/name').text\n",
    "        \n",
    "        img_path = os.path.join(images_folder, breed_folder, annotation_file + '.jpg')\n",
    "        image = cv2.imread(img_path)\n",
    "        image = cv2.resize(image, (224, 224))  # Resize to desired size\n",
    "        image = image/255\n",
    "        \n",
    "        images.append(image)\n",
    "        labels.append(breed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "759956be-f330-4036-a157-027bc093dba8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20580\n"
     ]
    }
   ],
   "source": [
    "# Convert labels to numerical format\n",
    "no_of_labels = (len(labels))\n",
    "print(no_of_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d7dc771d-be57-48cc-b81b-21d1e9de887c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20580, 1)\n"
     ]
    }
   ],
   "source": [
    "label_encoder = LabelEncoder()\n",
    "labels_encoded = label_encoder.fit_transform(labels)\n",
    "num_classes = len(label_encoder.classes_)\n",
    "labels_encoded=labels_encoded.reshape(no_of_labels, 1)\n",
    "print(labels_encoded.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e6530bf8-ba7b-4f58-884d-7945b0954dd2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20580, 120)\n"
     ]
    }
   ],
   "source": [
    "# Preprocess the labels\n",
    "labels_encoded_onehot = to_categorical(labels_encoded, num_classes)\n",
    "print(labels_encoded_onehot.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f7d2e45e-0567-4ed9-b7b7-19f223ad5fab",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3137254901960784\n"
     ]
    }
   ],
   "source": [
    "# Convert the images list to a NumPy array\n",
    "images = np.array(images)\n",
    "# print(images[0].shape)\n",
    "print(images[0][1][223][0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "28ea8cf9-3aea-483d-ac57-e656ad23e2f2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First row of labels_encoded_onehot:\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Shape of labels_encoded_onehot: (20580, 120)\n"
     ]
    }
   ],
   "source": [
    "# Print the first row\n",
    "print(\"First row of labels_encoded_onehot:\")\n",
    "print(labels_encoded_onehot[0])\n",
    "\n",
    "# Print the shape of the array\n",
    "print(\"Shape of labels_encoded_onehot:\", labels_encoded_onehot.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "20cc1365-e55d-46bb-a980-ead700419501",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20580, 120)\n",
      "(20580, 224, 224, 3)\n"
     ]
    }
   ],
   "source": [
    "print((labels_encoded_onehot.shape))\n",
    "print((images.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8b3128a7-ba2a-467d-bb17-f1ffc8f95c36",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 3.46 GiB for an array with shape (3087, 224, 224, 3) and data type float64",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Split data into training, validation, and testing sets\u001b[39;00m\n\u001b[0;32m      2\u001b[0m X_train, X_temp, y_train, y_temp \u001b[38;5;241m=\u001b[39m train_test_split(images, labels_encoded_onehot, test_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.3\u001b[39m, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m)\n\u001b[1;32m----> 3\u001b[0m X_val, X_test, y_val, y_test \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_test_split\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_temp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_temp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrandom_state\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m42\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m((y_train)\u001b[38;5;241m.\u001b[39mshape)\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m((X_train)\u001b[38;5;241m.\u001b[39mshape)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\sklearn\\model_selection\\_split.py:2585\u001b[0m, in \u001b[0;36mtrain_test_split\u001b[1;34m(test_size, train_size, random_state, shuffle, stratify, *arrays)\u001b[0m\n\u001b[0;32m   2581\u001b[0m     cv \u001b[38;5;241m=\u001b[39m CVClass(test_size\u001b[38;5;241m=\u001b[39mn_test, train_size\u001b[38;5;241m=\u001b[39mn_train, random_state\u001b[38;5;241m=\u001b[39mrandom_state)\n\u001b[0;32m   2583\u001b[0m     train, test \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(cv\u001b[38;5;241m.\u001b[39msplit(X\u001b[38;5;241m=\u001b[39marrays[\u001b[38;5;241m0\u001b[39m], y\u001b[38;5;241m=\u001b[39mstratify))\n\u001b[1;32m-> 2585\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2586\u001b[0m \u001b[43m    \u001b[49m\u001b[43mchain\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_iterable\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2587\u001b[0m \u001b[43m        \u001b[49m\u001b[43m(\u001b[49m\u001b[43m_safe_indexing\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_safe_indexing\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43ma\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43marrays\u001b[49m\n\u001b[0;32m   2588\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2589\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\sklearn\\model_selection\\_split.py:2587\u001b[0m, in \u001b[0;36m<genexpr>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m   2581\u001b[0m     cv \u001b[38;5;241m=\u001b[39m CVClass(test_size\u001b[38;5;241m=\u001b[39mn_test, train_size\u001b[38;5;241m=\u001b[39mn_train, random_state\u001b[38;5;241m=\u001b[39mrandom_state)\n\u001b[0;32m   2583\u001b[0m     train, test \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(cv\u001b[38;5;241m.\u001b[39msplit(X\u001b[38;5;241m=\u001b[39marrays[\u001b[38;5;241m0\u001b[39m], y\u001b[38;5;241m=\u001b[39mstratify))\n\u001b[0;32m   2585\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(\n\u001b[0;32m   2586\u001b[0m     chain\u001b[38;5;241m.\u001b[39mfrom_iterable(\n\u001b[1;32m-> 2587\u001b[0m         (_safe_indexing(a, train), \u001b[43m_safe_indexing\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest\u001b[49m\u001b[43m)\u001b[49m) \u001b[38;5;28;01mfor\u001b[39;00m a \u001b[38;5;129;01min\u001b[39;00m arrays\n\u001b[0;32m   2588\u001b[0m     )\n\u001b[0;32m   2589\u001b[0m )\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\sklearn\\utils\\__init__.py:356\u001b[0m, in \u001b[0;36m_safe_indexing\u001b[1;34m(X, indices, axis)\u001b[0m\n\u001b[0;32m    354\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _pandas_indexing(X, indices, indices_dtype, axis\u001b[38;5;241m=\u001b[39maxis)\n\u001b[0;32m    355\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(X, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mshape\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m--> 356\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_array_indexing\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindices\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindices_dtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    357\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    358\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _list_indexing(X, indices, indices_dtype)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\sklearn\\utils\\__init__.py:185\u001b[0m, in \u001b[0;36m_array_indexing\u001b[1;34m(array, key, key_dtype, axis)\u001b[0m\n\u001b[0;32m    183\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[0;32m    184\u001b[0m     key \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(key)\n\u001b[1;32m--> 185\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43marray\u001b[49m\u001b[43m[\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m axis \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m array[:, key]\n",
      "\u001b[1;31mMemoryError\u001b[0m: Unable to allocate 3.46 GiB for an array with shape (3087, 224, 224, 3) and data type float64"
     ]
    }
   ],
   "source": [
    "# Split data into training, validation, and testing sets\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(images, labels_encoded_onehot, test_size=0.3, random_state=42)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n",
    "\n",
    "print((y_train).shape)\n",
    "print((X_train).shape)\n",
    "print((y_val).shape)\n",
    "print((X_val).shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72cfb460-5117-4b87-8a40-d5dad6a974a7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Build Simple CNN\n",
    "def build_simple_cnn(input_shape, num_classes):\n",
    "    model = Sequential()\n",
    "    \n",
    "    # Convolutional blocks\n",
    "    for _ in range(5):\n",
    "        model.add(Conv2D(64, (3, 3), padding='same', input_shape=input_shape))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(ReLU())\n",
    "        model.add(MaxPooling2D((2, 2)))\n",
    "        model.add(Dropout(0.25))\n",
    "    \n",
    "    # Flatten and fully connected layers\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(128, activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(num_classes, activation='softmax'))\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e938ee0-9500-4b9e-a98b-db2374e87117",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Build Xception model with transfer learning\n",
    "def build_xception(input_shape, num_classes):\n",
    "    base_model = Xception(weights='imagenet', include_top=False, input_shape=input_shape)\n",
    "    for layer in base_model.layers:\n",
    "        layer.trainable = False\n",
    "    model = Sequential()\n",
    "    model.add(base_model)\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(num_classes, activation='softmax'))\n",
    "    return model\n",
    "\n",
    "# Specify input shape and number of classes\n",
    "input_shape = (224, 224, 3)  # Example input shape\n",
    "\n",
    "# Build and compile Simple CNN model\n",
    "simple_cnn_model = build_simple_cnn(input_shape, num_classes)\n",
    "simple_cnn_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Build and compile Xception model\n",
    "xception_model = build_xception(input_shape, num_classes)\n",
    "xception_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# # Data augmentation for Simple CNN\n",
    "# datagen = tf.keras.preprocessing.image.ImageDataGenerator(\n",
    "#     horizontal_flip=True,\n",
    "#     rotation_range=20,\n",
    "#     width_shift_range=0.2,\n",
    "#     height_shift_range=0.2,\n",
    "#     shear_range=0.2,\n",
    "#     zoom_range=0.2,\n",
    "#     fill_mode='nearest',\n",
    "#     preprocessing_function=tf.keras.applications.xception.preprocess_input\n",
    "# )\n",
    "\n",
    "# # Create data generators using tf.data\n",
    "# def create_data_generator(images, labels, batch_size, is_training=True):\n",
    "#     dataset = tf.data.Dataset.from_tensor_slices((images, labels))\n",
    "#     if is_training:\n",
    "#         dataset = dataset.shuffle(buffer_size=len(images)).repeat()\n",
    "\n",
    "#     dataset = dataset.map(lambda x, y: (datagen.random_transform(x), y), num_parallel_calls=tf.data.AUTOTUNE)\n",
    "#     dataset = dataset.batch(batch_size).prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    "#     return dataset\n",
    "\n",
    "# # Create data generators\n",
    "# train_data_generator = create_data_generator(X_train, y_train_onehot, batch_size=32)\n",
    "# val_data_generator = create_data_generator(X_val, y_val_onehot, batch_size=32, is_training=False)\n",
    "# test_data_generator = create_data_generator(X_test, y_test_onehot, batch_size=32, is_training=False)\n",
    "\n",
    "# Train Simple CNN\n",
    "history_simple_cnn = simple_cnn_model.fit(X_train, y_train, epochs=5, validation_data=(X_val, y_val))\n",
    "\n",
    "# Train Xception\n",
    "history_xception = xception_model.fit(X_train, y_train, epochs=5, validation_data=(X_val, y_val))\n",
    "\n",
    "# Evaluate models\n",
    "simple_cnn_loss, simple_cnn_acc = simple_cnn_model.evaluate(X_test, y_test, verbose=2)\n",
    "xception_loss, xception_acc = xception_model.evaluate(X_test, y_test, verbose=2)\n",
    "\n",
    "print(\"Simple CNN - Test accuracy:\", simple_cnn_acc)\n",
    "print(\"Xception - Test accuracy:\", xception_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8b4e0b6-b978-4608-80ae-87f15235958e",
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_cnn_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87d03298-ee85-4dc9-ab94-992bac375ed4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "xception_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61a83697-d149-4250-8669-1a9cf1abbc79",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Plot accuracy and loss curves for Simple CNN and Xception\n",
    "def plot_training_curves(history, title):\n",
    "    plt.figure(figsize=(12, 4))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(history.history['accuracy'], label='train')\n",
    "    plt.plot(history.history['val_accuracy'], label='validation')\n",
    "    plt.title(title + ' Accuracy')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(history.history['loss'], label='train')\n",
    "    plt.plot(history.history['val_loss'], label='validation')\n",
    "    plt.title(title + ' Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "plot_training_curves(history_simple_cnn, 'Simple CNN')\n",
    "plot_training_curves(history_xception, 'Xception')\n",
    "\n",
    "# Load and preprocess a new image\n",
    "new_image_path = r'C:\\Users\\haris\\AI\\ML\\Projects\\Data\\Dog Breed Classification\\images - Copy\\n02085936_37.jpg'\n",
    "new_image = cv2.imread(new_image_path)\n",
    "new_image = cv2.resize(new_image, (224, 224))\n",
    "new_image = new_image / 255.0\n",
    "new_image = np.expand_dims(new_image, axis=0)\n",
    "\n",
    "# Make predictions using the trained models\n",
    "simple_cnn_predictions = simple_cnn_model.predict(new_image)\n",
    "xception_predictions = xception_model.predict(new_image)\n",
    "\n",
    "# Decode the predictions\n",
    "simple_cnn_predicted_class = np.argmax(simple_cnn_predictions)\n",
    "xception_predicted_class = np.argmax(xception_predictions)\n",
    "\n",
    "simple_cnn_predicted_label = label_encoder.classes_[simple_cnn_predicted_class]\n",
    "xception_predicted_label = label_encoder.classes_[xception_predicted_class]\n",
    "\n",
    "simple_cnn_probabilities = simple_cnn_predictions[0]\n",
    "xception_probabilities = xception_predictions[0]\n",
    "\n",
    "# Print the predictions\n",
    "print(\"Simple CNN Predicted Class:\", simple_cnn_predicted_label)\n",
    "print(\"Simple CNN Predicted Probabilities:\", simple_cnn_probabilities)\n",
    "\n",
    "print(\"Xception Predicted Class:\", xception_predicted_label)\n",
    "print(\"Xception Predicted Probabilities:\", xception_probabilities)\n",
    "\n",
    "# Plot the predicted probabilities for the new image\n",
    "def plot_predicted_probabilities(classes, probabilities, title):\n",
    "    plt.figure(figsize=(8, 4))\n",
    "    plt.bar(classes, probabilities)\n",
    "    plt.title(title)\n",
    "    plt.xlabel('Classes')\n",
    "    plt.ylabel('Probabilities')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "class_names = label_encoder.classes_\n",
    "plot_predicted_probabilities(class_names, simple_cnn_probabilities, 'Simple CNN Predicted Probabilities')\n",
    "plot_predicted_probabilities(class_names, xception_probabilities, 'Xception Predicted Probabilities')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05a45057-622d-4be1-80d2-30d20ae22508",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10 (tensorflow-gpu)",
   "language": "python",
   "name": "tensorflow-gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
