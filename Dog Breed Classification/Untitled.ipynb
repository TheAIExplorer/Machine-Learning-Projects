{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8e205199-ae87-48bd-8575-ea150464a77d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20580\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "train_dir = r'C:\\Users\\haris\\AI\\ML\\Projects\\Data\\Dog Breed Classification\\images'\n",
    "dest_dir = r'C:\\Users\\haris\\AI\\ML\\Projects\\Data\\Dog Breed Classification\\all_images'\n",
    "counter = 0\n",
    "\n",
    "for subdir, dirs, files in os.walk(train_dir):\n",
    "    #print(files)\n",
    "    for file in files:\n",
    "        full_path = os.path.join(subdir, file)\n",
    "        shutil.copy(full_path, dest_dir)\n",
    "        counter = counter + 1\n",
    "print(counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fcaaad95-bac8-47cf-8e9b-1c4f8c429cfd",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20580\n",
      "20580\n",
      "(20580, 1)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "subdirs, dirs, files = os.walk(r'C:\\Users\\haris\\AI\\ML\\Projects\\Data\\Dog Breed Classification\\all_images').__next__()\n",
    "m = len(files)\n",
    "print(m)\n",
    "\n",
    "filenames = []\n",
    "labels = np.zeros((m, 1))\n",
    "\n",
    "images_dir = r'C:\\Users\\haris\\AI\\ML\\Projects\\Data\\Dog Breed Classification\\all_images'\n",
    "filenames_counter = 0\n",
    "labels_counter = -1\n",
    "\n",
    "for subdir, dirs, files in os.walk(train_dir):\n",
    "    #print(files)\n",
    "    for file in files:\n",
    "        filenames.append(file)\n",
    "        labels[filenames_counter, 0] = labels_counter\n",
    "        filenames_counter = filenames_counter + 1\n",
    "    labels_counter = labels_counter+1\n",
    "    \n",
    "print(len(filenames))\n",
    "print(labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ca906c6d-7e8a-4338-a458-2831e2a3855c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# saving the filename array as .npy file\n",
    "np.save('filenames.npy', filenames)\n",
    "\n",
    "import keras\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "# One hot vector representation of labels\n",
    "y_labels_one_hot = to_categorical(labels)\n",
    "\n",
    "# saving the y_labels_one_hot array as a .npy file\n",
    "np.save('y_labels_one_hot.npy', y_labels_one_hot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8f7dcf4c-e61b-436c-9a3c-f8b237c46ca1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.utils import shuffle\n",
    "\n",
    "filenames_shuffled, y_labels_one_hot_shuffled = shuffle(filenames, y_labels_one_hot)\n",
    "\n",
    "# saving the shuffled file.\n",
    "# you can load them later using np.load().\n",
    "np.save('y_labels_one_hot_shuffled.npy', y_labels_one_hot_shuffled)\n",
    "np.save('filenames_shuffled.npy', filenames_shuffled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5fd00a33-6a42-46fa-bd31-fdb8074a48f8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(16464,)\n",
      "(16464, 120)\n",
      "(4116,)\n",
      "(4116, 120)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Used this line as our filename array is not a numpy array.\n",
    "filenames_shuffled_numpy = np.array(filenames_shuffled)\n",
    "\n",
    "X_train_filenames, X_val_filenames, y_train, y_val = train_test_split(\n",
    "    filenames_shuffled_numpy, y_labels_one_hot_shuffled, test_size=0.2, random_state=1)\n",
    "\n",
    "print(X_train_filenames.shape) # (3800,)\n",
    "print(y_train.shape)           # (3800, 12)\n",
    "\n",
    "print(X_val_filenames.shape)   # (950,)\n",
    "print(y_val.shape)             # (950, 12)\n",
    "\n",
    "# You can save these files as well. As you will be using them later for training and validation of your model.\n",
    "np.save('X_train_filenames.npy', X_train_filenames)\n",
    "np.save('y_train.npy', y_train)\n",
    "\n",
    "np.save('X_val_filenames.npy', X_val_filenames)\n",
    "np.save('y_val.npy', y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ca907303-e589-45ee-8c48-0efc5e5cf9af",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class My_Custom_Generator(keras.utils.Sequence) :\n",
    "    \n",
    "    def __init__(self, image_filenames, labels, batch_size) :\n",
    "        self.image_filenames = image_filenames\n",
    "        self.labels = labels\n",
    "        self.batch_size = batch_size\n",
    "    \n",
    "    def __len__(self) :\n",
    "        return (np.ceil(len(self.image_filenames) / float(self.batch_size))).astype(np.int)\n",
    "\n",
    "    def __getitem__(self, idx) :\n",
    "        batch_x = self.image_filenames[idx * self.batch_size : (idx+1) * self.batch_size]\n",
    "        batch_y = self.labels[idx * self.batch_size : (idx+1) * self.batch_size]\n",
    "\n",
    "        return np.array([\n",
    "                resize(imread(r'C:/Users/haris/AI/ML/Projects/Data/Dog Breed Classification/all_images/' + str(file_name)), (224, 224, 3))\n",
    "                   for file_name in batch_x])/255.0, np.array(batch_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "85eee156-5075-4f13-bdab-10ae3fbf95f4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "\n",
    "my_training_batch_generator = My_Custom_Generator(X_train_filenames, y_train, batch_size)\n",
    "my_validation_batch_generator = My_Custom_Generator(X_val_filenames, y_val, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e1f66589-5ddc-4b59-9094-6f0a1426d101",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from skimage.io import imread\n",
    "from skimage.transform import resize\n",
    "\n",
    "import keras\n",
    "from keras.models import Sequential, Model, load_model\n",
    "from keras.layers import Input, Conv1D, Conv2D, MaxPooling1D, MaxPooling2D, Dense, Dropout, Activation, Flatten\n",
    "\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import cv2\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras import layers\n",
    "from functools import partial\n",
    "\n",
    "import os\n",
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, BatchNormalization, ReLU, MaxPooling2D, Dropout, Flatten, Dense\n",
    "from tensorflow.keras.applications import Xception\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "AUTO = tf.data.experimental.AUTOTUNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4297cae6-d32a-4429-b219-6fba54631d12",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Specify input shape and number of classes\n",
    "input_shape = (224, 224, 3)\n",
    "num_classes=120\n",
    "\n",
    "# Build Simple CNN\n",
    "def build_simple_cnn(input_shape, num_classes):\n",
    "    model = Sequential()\n",
    "    \n",
    "    # Convolutional blocks\n",
    "    for _ in range(5):\n",
    "        model.add(Conv2D(64, (3, 3), padding='same', input_shape=input_shape))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(ReLU())\n",
    "        model.add(MaxPooling2D((2, 2)))\n",
    "        model.add(Dropout(0.25))\n",
    "    \n",
    "    # Flatten and fully connected layers\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(128, activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(num_classes, activation='softmax'))\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72219700-7b1c-4711-a175-f77ca4722157",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "514/514 [==============================] - 467s 891ms/step - loss: 4.7999 - accuracy: 0.0093 - val_loss: 4.7848 - val_accuracy: 0.0132\n",
      "Epoch 2/10\n",
      "306/514 [================>.............] - ETA: 2:07 - loss: 4.7841 - accuracy: 0.0117"
     ]
    }
   ],
   "source": [
    "# Build Xception model with transfer learning\n",
    "def build_xception(input_shape, num_classes):\n",
    "    base_model = Xception(weights='imagenet', include_top=False, input_shape=input_shape)\n",
    "    for layer in base_model.layers:\n",
    "        layer.trainable = False\n",
    "    model = Sequential()\n",
    "    model.add(base_model)\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(num_classes, activation='softmax'))\n",
    "    return model\n",
    "\n",
    "# Build and compile Simple CNN model\n",
    "simple_cnn_model = build_simple_cnn(input_shape, num_classes)\n",
    "simple_cnn_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Build and compile Xception model\n",
    "xception_model = build_xception(input_shape, num_classes)\n",
    "xception_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# # Data augmentation for Simple CNN\n",
    "# datagen = tf.keras.preprocessing.image.ImageDataGenerator(\n",
    "#     horizontal_flip=True,\n",
    "#     rotation_range=20,\n",
    "#     width_shift_range=0.2,\n",
    "#     height_shift_range=0.2,\n",
    "#     shear_range=0.2,\n",
    "#     zoom_range=0.2,\n",
    "#     fill_mode='nearest',\n",
    "#     preprocessing_function=tf.keras.applications.xception.preprocess_input\n",
    "# )\n",
    "\n",
    "# # Create data generators using tf.data\n",
    "# def create_data_generator(images, labels, batch_size, is_training=True):\n",
    "#     dataset = tf.data.Dataset.from_tensor_slices((images, labels))\n",
    "#     if is_training:\n",
    "#         dataset = dataset.shuffle(buffer_size=len(images)).repeat()\n",
    "\n",
    "#     dataset = dataset.map(lambda x, y: (datagen.random_transform(x), y), num_parallel_calls=tf.data.AUTOTUNE)\n",
    "#     dataset = dataset.batch(batch_size).prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    "#     return dataset\n",
    "\n",
    "# # Create data generators\n",
    "# train_data_generator = create_data_generator(X_train, y_train_onehot, batch_size=32)\n",
    "# val_data_generator = create_data_generator(X_val, y_val_onehot, batch_size=32, is_training=False)\n",
    "# test_data_generator = create_data_generator(X_test, y_test_onehot, batch_size=32, is_training=False)\n",
    "\n",
    "# Train Simple CNN\n",
    "history_simple_cnn = simple_cnn_model.fit_generator(generator=my_training_batch_generator,\n",
    "                   steps_per_epoch = int(16464 // batch_size),\n",
    "                   epochs = 10,\n",
    "                   verbose = 1,\n",
    "                   validation_data = my_validation_batch_generator,\n",
    "                   validation_steps = int(4116 // batch_size))\n",
    "\n",
    "# Train Xception\n",
    "history_xception = xception_model.fit_generator(generator=my_training_batch_generator,\n",
    "                   steps_per_epoch = int(16464 // batch_size),\n",
    "                   epochs = 10,\n",
    "                   verbose = 1,\n",
    "                   validation_data = my_validation_batch_generator,\n",
    "                   validation_steps = int(4116 // batch_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34023581-ac62-46d6-bfa5-4da2f6e04aed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Evaluate models\n",
    "# simple_cnn_loss, simple_cnn_acc = simple_cnn_model.evaluate(X_test, y_test, verbose=2)\n",
    "# xception_loss, xception_acc = xception_model.evaluate(X_test, y_test, verbose=2)\n",
    "\n",
    "# print(\"Simple CNN - Test accuracy:\", simple_cnn_acc)\n",
    "# print(\"Xception - Test accuracy:\", xception_acc)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10 (tensorflow-gpu)",
   "language": "python",
   "name": "tensorflow-gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
